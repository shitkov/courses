{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_Embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfbLLXj-4lLp"
      },
      "source": [
        "# Семинар 3: Векторы слов\n",
        "\n",
        "Другие курсы на ту же тему:\n",
        "* https://github.com/DanAnastasyev/DeepNLP-Course: Курс Данила Анастасьева, Week 2\n",
        "* https://www.youtube.com/watch?v=ERibwqs9p38: Stanford CS224n, Lecture 2\n",
        "* https://github.com/deepmipt/deep-nlp-seminars: Курс DeepMIPT, Seminar 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tL_FPZK2jkM"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install -y python-setuptools python-pip\n",
        "!pip install --upgrade pybind11 setuptools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DK2BZThwFJdw"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "gensim\n",
        "pandas\n",
        "razdel\n",
        "hnswlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69T1Gng3842m"
      },
      "source": [
        "!pip install --upgrade -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk6w5CO_D-by"
      },
      "source": [
        "### Скачиваем датасет на сегодня"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O1IEMGm9l20"
      },
      "source": [
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz\n",
        "!gzip -d lenta-ru-news.csv.gz\n",
        "!head -n 2 lenta-ru-news.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLQ6DzCB40Jy"
      },
      "source": [
        "### Обрабатываем датасет"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Im2V1KH3EHCL"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv(\"lenta-ru-news.csv\", sep=',', quotechar='\\\"', escapechar='\\\\', encoding='utf-8', header=0)\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3aqjNZRF-fj"
      },
      "source": [
        "import re\n",
        "import datetime as dt\n",
        "\n",
        "def get_date(url):\n",
        "    dates = re.findall(r\"\\d\\d\\d\\d\\/\\d\\d\\/\\d\\d\", url)\n",
        "    return next(iter(dates), None)\n",
        "  \n",
        "dataset[\"date\"] = dataset[\"url\"].apply(lambda x: dt.datetime.strptime(get_date(x), \"%Y/%m/%d\"))\n",
        "dataset = dataset[dataset[\"date\"] > \"2017-01-01\"]\n",
        "dataset[\"text\"] = dataset[\"text\"].apply(lambda x: x.replace(\"\\xa0\", \" \"))\n",
        "dataset[\"title\"] = dataset[\"title\"].apply(lambda x: x.replace(\"\\xa0\", \" \"))\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi5NhP-1K7jx"
      },
      "source": [
        "train_dataset = dataset[dataset[\"date\"] < \"2018-04-01\"]\n",
        "test_dataset = dataset[dataset[\"date\"] > \"2018-04-01\"]\n",
        "print(train_dataset.info())\n",
        "print(test_dataset.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD2wHUVt5eAF"
      },
      "source": [
        "# Задачи, которые будем решать\n",
        "* Семантический поиск по заголовку\n",
        "* Рубрикация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grolwlqh4-sr"
      },
      "source": [
        "## Подготовка: разбиение на предложения и токенизация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2VWxHQgOTYo"
      },
      "source": [
        "from razdel import tokenize, sentenize\n",
        "from string import punctuation\n",
        "\n",
        "texts = []\n",
        "for text in train_dataset[\"text\"]:\n",
        "    pass\n",
        "    # Разбейте на предложения\n",
        "    # Каждое предложение токенизируйте и список токенов положите в texts.\n",
        "    # Токены приведите к нижнему регистру и избавьтесь от пунктуации.\n",
        "    \n",
        "for title in train_dataset[\"title\"]:\n",
        "    pass\n",
        "    # Считайте заголовок одним предложением\n",
        "\n",
        "assert len(texts) == 827217\n",
        "assert len(texts[0]) > 0\n",
        "assert texts[0][0].islower()\n",
        "print(texts[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqxeSd4G6X5G"
      },
      "source": [
        "## Коротко о Word2Vec\n",
        "Обучение:\n",
        "\n",
        "![embeddings training](https://miro.medium.com/max/1400/0*o2FCVrLKtdcxPQqc.png)\n",
        "*From [An implementation guide to Word2Vec using NumPy and Google Sheets\n",
        "](https://towardsdatascience.com/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281)*\n",
        "\n",
        "![embeddings relations](https://www.tensorflow.org/images/linear-relationships.png)\n",
        "*From [Vector Representations of Words, Tensorflow tutorial](https://www.tensorflow.org/tutorials/representation/word2vec)*\n",
        "\n",
        "Статьи:\n",
        "* Word2Vec: [Distributed Representations of Words and Phrases\n",
        "and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), Mikolov et al., 2013\n",
        "* GloVe: [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf), Pennington, Socher, Manning, 2014\n",
        "* fastText: [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf), Bojanowski, Grave, Joulin, Mikolov, 2016\n",
        "\n",
        "Ссылки:\n",
        "* Word2Vec и fasttext модели для русского: https://rusvectores.org/ru/\n",
        "* fasttext для кучи языков: https://fasttext.cc/\n",
        "* Ещё fasttext модели для русского: http://docs.deeppavlov.ai/en/master/features/pretrained_vectors.html\n",
        "* Отдельная библиотека для русских векторов: https://github.com/natasha/navec\n",
        "* Word2Vec для кучи языков, обученная на Вики: https://wikipedia2vec.github.io/wikipedia2vec/pretrained/\n",
        "* Word2Vec для английского от Гугла: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "* Огромная Word2Vec модель для русского: https://zenodo.org/record/400631#.Xa4RPN9fjCI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maSFl50k6DIv"
      },
      "source": [
        "## Тренируем простую модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i0bx5hBK5yn"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(texts, \n",
        "                 size=32,     # embedding vector size\n",
        "                 min_count=5,  # consider words that occured at least 5 times\n",
        "                 window=5).wv  # define context as a 5-word window around the target word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gK3c0mM0-RNs"
      },
      "source": [
        "Полноценная тренировка в следующий раз :)\n",
        "А теперь немного потестируем нашу модель."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdpt9Z7m-faj"
      },
      "source": [
        "## Тестируем модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUwJEbQVJ8B7"
      },
      "source": [
        "model.get_vector(\"сша\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZJgVbimJWfn"
      },
      "source": [
        "model.most_similar('сша')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "illnNmMTul7E"
      },
      "source": [
        "model.most_similar([model.get_vector('трамп') - model.get_vector('сша') + model.get_vector('россии')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apy3l1Xy_fX1"
      },
      "source": [
        "### Задание: Найдите свою аналогию\n",
        "Поиграйтесь с моделью и найдите свои аналогии. Можно здесь, можно на rusvectores\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znoMFNXL-n90"
      },
      "source": [
        "## Визуализируем векторы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJ9Xtl58Sbsq"
      },
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    output_notebook()\n",
        "    \n",
        "    if isinstance(color, str): \n",
        "        color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({'x' : x, 'y' : y, 'color': color, **kwargs})\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show:\n",
        "        pl.show(fig)\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNgPonL1KXsC"
      },
      "source": [
        "words = sorted(model.vocab.keys(), \n",
        "               key=lambda word: model.vocab[word].count,\n",
        "               reverse=True)[:1000]\n",
        "word_vectors = model.vectors[[model.vocab[word].index for word in words]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wySL7tlja0D8"
      },
      "source": [
        "### PCA\n",
        "\n",
        "Простейший линейный метод сокращения размерностей - __P__rincipial __C__omponent __A__nalysis.\n",
        "\n",
        "PCA ищет оси, при проекции на которые данные будут иметь наибольший разброс.\n",
        "\n",
        "![pca](https://i.stack.imgur.com/Q7HIP.gif)\n",
        "*From [https://stats.stackexchange.com/a/140579](https://stats.stackexchange.com/a/140579)*\n",
        "\n",
        "В результате, можно взять проекции на несколько первых компонент - и сохранить как можно больше информации, сократив размерность.\n",
        "\n",
        "Красивые визуализации можно найти [здесь](http://setosa.io/ev/principal-component-analysis/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlWUB1UsQ-NY"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca_model = PCA(n_components=2)\n",
        "pca_vectors = pca_model.fit_transform(word_vectors)\n",
        "pca_vectors = (pca_vectors - pca_vectors.mean(0)) / pca_vectors.std(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_6xKPpBSfWX"
      },
      "source": [
        "draw_vectors(pca_vectors[:, 0], pca_vectors[:, 1], token=words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFUUZyJM_DAa"
      },
      "source": [
        "Получилось не очень понятно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jET8tFuXbEKm"
      },
      "source": [
        "### TSNE\n",
        "\n",
        "Более интересный и сложный (нелинейный) метод для визуализации высокоразмерных пространств - TSNE. Подробно посмотреть на него можно [здесь](https://distill.pub/2016/misread-tsne/) (ещё более красивые картинки!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-X_8gVkR_GOa"
      },
      "source": [
        "### Задание: TSNE\n",
        "Сделайте то же самое, но с TSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8gvbFIlKTeAp"
      },
      "source": [
        "# CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuybLGbn_Ra_"
      },
      "source": [
        "draw_vectors(tsne_vectors[:, 0], tsne_vectors[:, 1], token=words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc0Tcni7_XZ9"
      },
      "source": [
        "## Поиск заголовков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-W0ag2rT2dE"
      },
      "source": [
        "from razdel import tokenize\n",
        "import numpy as np\n",
        "\n",
        "def get_text_embedding(model, phrase):\n",
        "    embeddings = np.array([model.get_vector(word.text.lower()) if word.text.lower() in model.vocab else np.zeros((model.vector_size,))\n",
        "                           for word in tokenize(phrase)])\n",
        "    return np.mean(embeddings, axis=0)\n",
        "    \n",
        "get_text_embedding(model, \"В Москве нашли\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDMIP-hRDPnc"
      },
      "source": [
        "### Задание: k ближайших заголовков\n",
        "Напишите функцию для поиска похожих на запрос заголовков\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpK83I49VMQ5"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_nearest(model, text_vectors, texts, query, k=10):\n",
        "    # YOUR CODE HERE\n",
        "    pass\n",
        "\n",
        "test_titles = test_dataset[\"title\"].tolist()\n",
        "title_vectors = np.array([get_text_embedding(model, title) for title in test_titles])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wx4OQahJ935Z"
      },
      "source": [
        "query = \"В Москве нашли\"\n",
        "near_titles = find_nearest(model, title_vectors, test_titles, query)\n",
        "assert len(near_titles) == 10\n",
        "\n",
        "near_titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRuvFP_d1o00"
      },
      "source": [
        "### HNSW-индекс\n",
        "\n",
        "Поиск за log(n)\n",
        "\n",
        "* https://github.com/nmslib/hnswlib\n",
        "* https://habr.com/ru/company/mailru/blog/338360/\n",
        "* https://arxiv.org/ftp/arxiv/papers/1603/1603.09320.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWMxBnwt1oNf"
      },
      "source": [
        "import hnswlib\n",
        "\n",
        "hnsw = hnswlib.Index(space='cosine', dim=title_vectors.shape[1])\n",
        "hnsw.init_index(max_elements=title_vectors.shape[0])\n",
        "hnsw.add_items(title_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sprIy3l77mb3"
      },
      "source": [
        "labels, distances = hnsw.knn_query(get_text_embedding(model, query), k=3)\n",
        "near_titles = [test_titles[i] for i in labels[0]]\n",
        "near_titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APIRPR0mDzuZ"
      },
      "source": [
        "## Рубрикация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lYlFzV1EjIF"
      },
      "source": [
        "target_labels = set(train_dataset[\"topic\"].dropna().tolist())\n",
        "target_labels -= {\"69-я параллель\", \"Крым\", \"Культпросвет \", \"Оружие\", \"Бизнес\", \"Путешествия\"}\n",
        "target_labels = list(target_labels)\n",
        "print(target_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d0iwB8MMYgP"
      },
      "source": [
        "pattern = r'(\\b{}\\b)'.format('|'.join(target_labels))\n",
        "\n",
        "train_with_topics = train_dataset[train_dataset[\"topic\"].str.contains(pattern, case=False, na=False)]\n",
        "train_with_topics = train_with_topics.head(20000)\n",
        "\n",
        "test_with_topics = test_dataset[test_dataset[\"topic\"].str.contains(pattern, case=False, na=False)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlFhWgoJZHlI"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "y_train = train_with_topics[\"topic\"].apply(lambda x: target_labels.index(x)).to_numpy()\n",
        "X_train = np.zeros((train_with_topics.shape[0], model.vector_size))\n",
        "for i, embedding in enumerate(train_with_topics[\"text\"]):\n",
        "    X_train[i, :] = get_text_embedding(model, embedding)\n",
        "\n",
        "y_test = test_with_topics[\"topic\"].apply(lambda x: target_labels.index(x)).to_numpy()\n",
        "X_test = np.zeros((test_with_topics.shape[0], model.vector_size))\n",
        "for i, embedding in enumerate(test_with_topics[\"text\"]):\n",
        "    X_test[i, :] = get_text_embedding(model, embedding)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWkHwB8RbwAv"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC()\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usVCxpg1dZDe"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "y_predicted = clf.predict(X_test)\n",
        "print(metrics.classification_report(y_test, y_predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wt-3D2s1Fgx1"
      },
      "source": [
        "### Задание: Больше точности\n",
        "\n",
        "Увеличить точность на 0.02+ на той же Word2Vec модели (например, используя tf-idf при построении вектора текста)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6kFWzIlGflX"
      },
      "source": [
        "## Предобученные векторы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdIj4x5TTUSH"
      },
      "source": [
        "### Задание: Модели rusvectores\n",
        "Используя fastText модели с rusvectores, достигните хотя бы такой же точности рубрикации"
      ]
    }
  ]
}