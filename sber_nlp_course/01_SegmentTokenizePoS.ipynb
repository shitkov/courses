{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"celltoolbar":"Raw Cell Format","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"colab":{"name":"01_SegmentTokenizePoS.ipynb","provenance":[],"collapsed_sections":["0zmDYwWleDZt","KQq4iPN8eDZ2","DAbQc-VzeDab","UmRbpXNTB72C","EvFlNWZ1eDbA","DPMiIDMEeDbE","dop3xhkteDbN","gqd_IVNNeDbS","BN71xIw6eDbY","NDIt0zSKeDbg","PhBHsBTyeDbo"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"scB5smameDYo"},"source":["# Настройка\n","Все зависимости перечислены в ячейке ниже. Кроме того, есть ещё дополнительные данные (opencorpora, например). Они тоже скачиваются в первых ячейках."]},{"cell_type":"code","metadata":{"id":"-PoyIMujeDYp"},"source":["%%writefile requirements.txt\n","nltk>=3.4.5\n","razdel>=0.4.0\n","rusenttokenize>=0.0.5\n","b-labs-models>=2017.8.22\n","lxml>=4.2.1\n","spacy>=2.1.4\n","pymystem3>=0.2.0\n","rnnmorph>=0.4.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_3ysaxzVeDYt"},"source":["import sys\n","!pip install --user --upgrade --force-reinstall -r requirements.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ocGvOjsE9GKh"},"source":["# Restart kernel\n","\n","import os\n","os._exit(0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FJ2-w6keDYv"},"source":["import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Dqa3TH-eDYx"},"source":["import sys\n","!python -m spacy download en_core_web_sm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5A7fL9saeDYz"},"source":["### Дополнительные данные\n","\n","Opencorpora: 31 Мб по сети, 530 Мб в распакованном виде"]},{"cell_type":"code","metadata":{"id":"1BnW_XqBeDY0"},"source":["!wget http://opencorpora.org/files/export/annot/annot.opcorpora.xml.bz2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6wX8NtUveDY2"},"source":["!bzip2 -d annot.opcorpora.xml.bz2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FiKkGUnbeDY4"},"source":["### Тестовые примеры"]},{"cell_type":"code","metadata":{"id":"UdqD-e7beDY5"},"source":["example1 = \"this's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\"\n","example2 = \"\"\"\n","    An ambitious campus expansion plan was proposed by Fr. Vernon F. Gallagher in 1952.\n","    Assumption Hall, the first student dormitory, was opened in 1954,\n","    and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.\n","    It was during the tenure of F. Henry J. McAnulty that Fr. Gallagher's ambitious plans were put to action.\n","\"\"\"\n","example3 = \"\"\"\n","    А что насчёт русского языка? Хорошо ли сегментируются имена?\n","    Ай да А.С. Пушкин! Ай да сукин сын!\n","    «Как же так?! Захар...» — воскликнут Пронин.\n","    - \"Так в чем же дело?\" - \"Не ра-ду-ют\".\n","    И т. д. и т. п. В общем, вся газета.\n","    Православие... более всего подходит на роль такой идеи...\n","    Нефть за $27/барр. не снится.\n","\"\"\"\n","example4 = \"\"\"\n","    Кружка-термос на 0.5л (50/64 см³, 516;...) стоит $3.88\n","\"\"\"\n","example5 = \"\"\"\n","    Good muffins cost $3.88 in New York.  Please buy me two of them. Thanks.\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2cqtzsw4eDY7"},"source":["# Сегментация предложений\n","Первая задача - разбиение текста на предложения"]},{"cell_type":"markdown","metadata":{"id":"NGebcOPseDY8"},"source":["### Экперименты"]},{"cell_type":"markdown","metadata":{"id":"LiEkV_MdeDY9"},"source":["##### NLTK - Natural Language Toolkit\n","Популярная платформа для анализа текстов. Особенно хорошо работает для английского. В основном не содержит ничего из машинного обучения, только старые добрые правила."]},{"cell_type":"code","metadata":{"id":"42DWKe5FeDY9"},"source":["from nltk.tokenize import sent_tokenize\n","sent_tokenize(example1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gWpr39pPeDZA"},"source":["А вот тут что-то пошло не так"]},{"cell_type":"code","metadata":{"id":"B8bXtc-6eDZB"},"source":["sent_tokenize(example2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0dpmBe70eDZE"},"source":["А что насчёт русского языка?"]},{"cell_type":"code","metadata":{"id":"MUNCCwnzeDZF"},"source":["sent_tokenize(example3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3rWZ0bF4eDZH"},"source":["https://github.com/Mottl/ru_punkt\n","\n","Data for sentence tokenization was taken from 3 sources:\n","\n","  * Articles from Russian Wikipedia (about 1 million sentences)\n","  * Common Russian abbreviations from Russian orthographic dictionary, edited by V. V. Lopatin;\n","  * Generated names initials."]},{"cell_type":"code","metadata":{"id":"Nwr9BaZ3eDZI"},"source":["sent_tokenize(example3, language=\"russian\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"erWAIVigeDZL"},"source":["https://github.com/natasha/razdel\n","\n","razdel старается разбивать текст на предложения и токены так, как это сделано в 4 датасетах: SynTagRus, OpenCorpora, ГИКРЯ и РНК из репозитория morphoRuEval-2017.\n","\n","В основном это новостные тексты и литература. Правила razdel заточены под них.\n","\n","На текстах другой тематики (социальные сети, научные статьи) библиотека может работать хуже."]},{"cell_type":"code","metadata":{"id":"iY1qpHh6eDZL"},"source":["from razdel import sentenize\n","list(sentenize(example3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v6DDGryLeDZP"},"source":["https://github.com/deepmipt/ru_sentence_tokenizer\n","    \n","A simple and fast rule-based sentence segmentation. Tested on OpenCorpora and SynTagRus datasets."]},{"cell_type":"code","metadata":{"id":"x6iYyZ1TeDZQ"},"source":["\n","from rusenttokenize import ru_sent_tokenize\n","ru_sent_tokenize(example3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pwNAtibneDZS"},"source":["### Бенчмарки\n","Много вариантов... Нужно измерять"]},{"cell_type":"code","metadata":{"id":"Pwae8cmleDZT"},"source":["# WARNING: RAM bound task, XML parsing is expensive\n","# Similar to https://github.com/deepmipt/ru_sentence_tokenizer/blob/master/metrics/calculate.ipynb\n","import re\n","from lxml import etree\n","\n","# \\W -> Any non-word character\n","RE_ENDS_WITH_PUNCT = re.compile(r\".*\\W$\")\n","\n","OPENCORPORA_FILE = \"annot.opcorpora.xml\"\n","sentences = list(etree.parse(OPENCORPORA_FILE).xpath('//source/text()'))\n","singles = []\n","compounds = []\n","s2 = sentences.pop().strip()\n","singles.append(s2)\n","while sentences:\n","    s1 = sentences.pop().strip()\n","    singles.append(s1)\n","    if RE_ENDS_WITH_PUNCT.match(s1) and not s1.endswith(':') and not s2.startswith('—'):\n","        compounds.append((s1, s2))\n","    s2 = s1\n","        \n","print(f'Read {len(singles)} sentences from {OPENCORPORA_FILE}')\n","        \n","del sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2havDX5seDZV"},"source":["def check_sent_tokenizer(tokenizer, singles, compounds):\n","    correct_count_in_singles = 0\n","    for sentence in singles:\n","        correct_count_in_singles += len(tokenizer(sentence)) == 1\n","\n","    correct_count_in_compounds = 0\n","    for s1, s2 in compounds:\n","        correct_count_in_compounds += tokenizer(s1 + ' ' + s2) == [s1, s2]\n","\n","    return (correct_count_in_singles / len(singles), correct_count_in_compounds / len(compounds))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kvWnP6bEeDZY"},"source":["from nltk.tokenize import sent_tokenize\n","%time singles_score, compounds_score = check_sent_tokenizer(sent_tokenize, singles, compounds)\n","print(f'sent_tokenizer scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9f9m7nnneDZc"},"source":["russian_sent_tokenize = lambda s : sent_tokenize(s, language=\"russian\")\n","%time singles_score, compounds_score = check_sent_tokenizer(russian_sent_tokenize, singles, compounds)\n","print(f'russian sent_tokenizer scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MClqfLRveDZh"},"source":["from razdel import sentenize\n","razdel_sent_tokenize = lambda text : [s.text for s in sentenize(text)]\n","%time singles_score, compounds_score = check_sent_tokenizer(razdel_sent_tokenize, singles, compounds)\n","print(f'razdel scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IuAOcawmeDZo"},"source":["from rusenttokenize import ru_sent_tokenize\n","deepmipt_sent_tokenize = ru_sent_tokenize\n","%time singles_score, compounds_score = check_sent_tokenizer(deepmipt_sent_tokenize, singles, compounds)\n","print(f'deepmipt scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0tQJPC51eDZs"},"source":["Аналогичные бенчмарки:\n","- https://github.com/natasha/razdel/blob/master/eval.ipynb\n","- https://github.com/deepmipt/ru_sentence_tokenizer/blob/master/metrics/calculate.ipynb"]},{"cell_type":"markdown","metadata":{"id":"0zmDYwWleDZt"},"source":["### Задание 1: \"Кирпич\"\n","Скачайте предложенный текст. Найдите первое предложение, которое отличается в разбиениях, порождённых rusenttokenize и razdel. Верните номер этого предложения."]},{"cell_type":"code","metadata":{"id":"uKi00K-seDZu"},"source":["!wget https://www.dropbox.com/s/q5wo34gfbepc7am/htbg.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oLzXHecqeDZy"},"source":["from razdel import sentenize\n","from rusenttokenize import ru_sent_tokenize\n","\n","\n","with open(\"htbg.txt\", \"r\") as f:\n","    text = f.read()\n","    \n","def get_first_different_sentence(text: str) -> int:\n","    # YOUR CODE HERE\n","    return -1\n","\n","assert get_first_different_sentence(text) == 329"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KQq4iPN8eDZ2"},"source":["### Задание 2: Lazy baseline\n","Напишите свой sent_tokenize, который будет делить предложения только по точкам, восклицательным и вопросительным знакам. Измерьте для него время работы и метрики на opencorpora."]},{"cell_type":"code","metadata":{"id":"47iRVTmqeDZ3"},"source":["from nltk.tokenize import sent_tokenize\n","def my_sent_tokenize(text):\n","    # YOUR CODE HERE\n","    return []\n","\n","assert my_sent_tokenize(example1) == sent_tokenize(example1)\n","%time singles_score, compounds_score = 0.0, 0.0 # YOUR CODE HERE\n","assert singles_score >= 0.85\n","print(f'your scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bvte2OBUeDZ5"},"source":["# Токенизация\n","\n","Самый наивный способ токенизировать текст -- разделить с помощью split. Но split упускает очень много всего, например, банально не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем. Поэтому лучше использовать готовые токенизаторы."]},{"cell_type":"code","metadata":{"id":"q0BjSKCneDZ5"},"source":["from nltk.tokenize import word_tokenize\n","print(word_tokenize(example5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"30jl5PpZeDZ8"},"source":["from nltk import tokenize\n","dir(tokenize)[:16]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eD3EUO5WeDZ_"},"source":["Они умеют выдавать индексы начала и конца каждого токена:"]},{"cell_type":"code","metadata":{"id":"Q2z_ZybxeDaA"},"source":["from nltk import tokenize\n","wh_tok = tokenize.WhitespaceTokenizer()\n","print(list(wh_tok.span_tokenize(example5)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D4HjdWCceDaE"},"source":["Некторые токенизаторы ведут себя специфично:"]},{"cell_type":"code","metadata":{"id":"OyzF35cOeDaG"},"source":["tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQCmuQpGeDaK"},"source":["import spacy\n","spacy_nlp = spacy.load('en_core_web_sm')\n","doc = spacy_nlp(example5, disable=[\"parser\"])\n","print([token.text for token in doc])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yg9w4wNSeDaN"},"source":["from nltk.tokenize import word_tokenize\n","print(word_tokenize(example4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVvJnHfKeDaP"},"source":["from razdel import tokenize\n","list(tokenize(example4))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IVCUdgpOeDaR"},"source":["### Задание 3: Diff\n","Напишите функцию, которая будет выводить разницу между токенизацией razdel'а и nltk."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"gRApSXtKeDaS"},"source":["from difflib import SequenceMatcher # USE THIS\n","from razdel import tokenize\n","from nltk.tokenize import word_tokenize\n","\n","with open(\"htbg.txt\", \"r\") as f:\n","    text = f.read()\n","    \n","def get_tokenization_differences(text: str) -> int:\n","    differences = []\n","    # YOUR CODE HERE\n","    return differences\n","\n","assert len(get_tokenization_differences(text)) == 613"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4IPBbpoVeDaU"},"source":["# Стоп-слова и пунктуация\n","\n","Стоп-слова - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конкретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."]},{"cell_type":"code","metadata":{"id":"4CwruKoseDaU"},"source":["from nltk.corpus import stopwords\n","print(stopwords.words('russian'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3PVbyxz-eDaW"},"source":["from string import punctuation\n","punctuation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7rksBLbveDaZ"},"source":["noise = stopwords.words('russian') + list(punctuation)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DAbQc-VzeDab"},"source":["### Задание 4: Стоп-слова from scratch\n","Постройте свой список стоп-слов на основе Opencorpora"]},{"cell_type":"code","metadata":{"id":"yGRerPDseDac"},"source":["import re\n","from lxml import etree\n","\n","OPENCORPORA_FILE = \"annot.opcorpora.xml\"\n","sentences = list(etree.parse(OPENCORPORA_FILE).xpath('//source/text()'))\n","print(f'Read {len(sentences)} sentences from {OPENCORPORA_FILE}')\n","\n","# YOUR CODE HERE\n"," \n","del sentences"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MkGQgO59KZ9G"},"source":["# Стемминг"]},{"cell_type":"code","metadata":{"id":"IZsLS4NQKW77"},"source":["from nltk.stem.snowball import SnowballStemmer \n","from razdel import tokenize\n","\n","stemmer = SnowballStemmer(\"russian\") \n","print([stemmer.stem(token.text) for token in tokenize(example3)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FM6ufubEeDae"},"source":["# Лемматизация и морфологический анализ\n","\n","Лемматизация – это сведение разных форм одного слова к начальной форме – лемме. Почему это хорошо?\n","* Мы хотим рассматривать как отдельную фичу каждое слово, а не каждую его отдельную форму.\n","* Некоторые стоп-слова стоят только в начальной форме, и без лемматизации выкидываем мы только её.\n","\n","Для русского есть два хороших лемматизатора: mystem и pymorphy. С pymorphy всё сразу понятно.\n","\n","Но как работать с Mystem:\n","* Можно скачать mystem и запускать из терминала с разными параметрами\n","* pymystem3 - обертка для питона, работает медленнее, но это удобно"]},{"cell_type":"markdown","metadata":{"id":"Mfork2jueDaf"},"source":["## Mystem"]},{"cell_type":"code","metadata":{"id":"O1ZoKAS5eDag"},"source":["from pymystem3 import Mystem\n","mystem_analyzer = Mystem()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7McqUjduMYxR"},"source":["!chmod +x /root/.local/bin/mystem"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XsSzbzVWeDai"},"source":["Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n","\n","    mystem_bin - путь к mystem, если их несколько\n","    grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n","    disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n","    entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n","\n","Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n","\n","Можно просто лемматизировать текст:"]},{"cell_type":"code","metadata":{"id":"uK6iAkmWeDai"},"source":["print(mystem_analyzer.lemmatize(example3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MZg3McyqeDal"},"source":["А можно получить грамматическую информацию:"]},{"cell_type":"code","metadata":{"id":"-F_PNsqneDal"},"source":["mystem_analyzer.analyze(example3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tJH0dy1YeDan"},"source":["## Pymorphy\n","\n","Это модуль на питоне, довольно быстрый и с кучей функций."]},{"cell_type":"code","metadata":{"id":"h5tsjLpjeDao"},"source":["from pymorphy2 import MorphAnalyzer\n","pymorphy2_analyzer = MorphAnalyzer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yOudNTMJeDap"},"source":["pymorphy2_analyzer.parse(\"мою\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UmRbpXNTB72C"},"source":["### Задание 5: Анализ частей речи\n","\n","Используя pymorphy2, определите топ-10 самых частотных существительных и глаголов в тексте"]},{"cell_type":"code","metadata":{"id":"My2CZHRwCODO"},"source":["from pymorphy2 import MorphAnalyzer\n","\n","pymorphy2_analyzer = MorphAnalyzer()\n","with open(\"htbg.txt\", \"r\") as f:\n","    text = f.read()\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o8qJvkxveDas"},"source":["## mystem vs. pymorphy\n","\n","1) Mystem работает невероятно медленно под windows на больших текстах.\n","\n","2) Снятие омонимии. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту:\n"]},{"cell_type":"code","metadata":{"id":"GO55wLFkeDas"},"source":["homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n","homonym2 = 'Сорока своровала блестящее украшение со стола.'\n","mystem_analyzer = Mystem() # инициализирую объект с дефолтными параметрами\n","\n","print(mystem_analyzer.analyze(homonym1)[-5])\n","print(mystem_analyzer.analyze(homonym2)[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GApdUILveDax"},"source":["## Rnnmorph\n","Обёртка над pymorphy с разрешением омонимии\n","\n","https://github.com/IlyaGusev/rnnmorph\n","\n","https://habr.com/ru/post/339954/"]},{"cell_type":"code","metadata":{"id":"0nUqNO1leDa8"},"source":["from rnnmorph.predictor import RNNMorphPredictor\n","from razdel import tokenize\n","\n","predictor = RNNMorphPredictor(language=\"ru\")\n","homonym = \"Косил косой косой косой\"\n","print(predictor.predict([t.text for t in tokenize(homonym)])[1])\n","print(predictor.predict([t.text for t in tokenize(homonym)])[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qeFOWPD17PC3"},"source":["## GramEval-2020\n","\n","Соревнование по определению морфологических характеристик, определению синтаксических зависимостей и лемматизации. Готовых инструментов не получилось, но весь код всех конкурсантов доступен.\n","* https://github.com/dialogue-evaluation/GramEval2020"]},{"cell_type":"markdown","metadata":{"id":"EvFlNWZ1eDbA"},"source":["### Задание 6: Формат\n","\n","Используя стандартные инструменты переведите корпус htbg.txt в формат CoNLL-U.\n","Используйте следующие колонки: \n","    1. Номер предложения в тексте\n","    2. Токен в том виде, в котором он встретился в тексте\n","    3. Лемма токена\n","    4. POS-таг токена\n","    5. Вектор грамматических значений токена\n","    6. Целевая метка (сделайте метку везде OUT)"]},{"cell_type":"markdown","metadata":{"id":"Qfew_CvgeDbB"},"source":["# Regex 101"]},{"cell_type":"code","metadata":{"id":"CDYSLv67eDbB"},"source":["import re"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DPMiIDMEeDbE"},"source":["#### match\n","ищет по заданному шаблону в начале строки"]},{"cell_type":"code","metadata":{"id":"OJ4RY9dveDbF"},"source":["result = re.match('ab+c.', 'abcdefghijkabcabc') # ищем по шаблону 'ab+c.' \n","print (result) # совпадение найдено:"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S2qN9Q1keDbH"},"source":["print(result.group(0)) # выводим найденное совпадение"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tqg4wISLeDbK"},"source":["result = re.match('abc.', 'abdefghijkabcabc')\n","print(result) # совпадение не найдено"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dop3xhkteDbN"},"source":["#### search\n","ищет по всей строке, возвращает только первое найденное совпадение"]},{"cell_type":"code","metadata":{"id":"PsP6-GN-eDbO"},"source":["result = re.search('ab+c.', 'aefgabchijkabcabc') \n","print(result) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gqd_IVNNeDbS"},"source":["#### findall\n","возвращает список всех найденных совпадений"]},{"cell_type":"code","metadata":{"id":"Ox0yWO57eDbT"},"source":["result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d4ncd6YPeDbV"},"source":["Вопросы: \n","1) почему нет последнего abc?\n","2) почему нет abcx?"]},{"cell_type":"markdown","metadata":{"id":"BN71xIw6eDbY"},"source":["#### split\n","разделяет строку по заданному шаблону\n"]},{"cell_type":"code","metadata":{"id":"jsWWmhUKeDbZ"},"source":["result = re.split(',', 'itsy, bitsy, teenie, weenie') \n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tGR8y8uCeDbc"},"source":["можно указать максимальное количество разбиений"]},{"cell_type":"code","metadata":{"id":"alEOle-XeDbd"},"source":["result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit = 2) \n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NDIt0zSKeDbg"},"source":["#### sub\n","ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n","\n","параметры: (pattern, repl, string)"]},{"cell_type":"code","metadata":{"id":"d79CKEy5eDbk"},"source":["result = re.sub('a', 'b', 'abcabc')\n","print (result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PhBHsBTyeDbo"},"source":["#### compile\n","компилирует регулярное выражение в отдельный объект"]},{"cell_type":"code","metadata":{"id":"6r3PuZ3feDbp"},"source":["# Пример: построение списка всех слов строки:\n","prog = re.compile('[А-Яа-яё\\-]+')\n","prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_gSGRhKeDbu"},"source":["# Ваш код"],"execution_count":null,"outputs":[]}]}