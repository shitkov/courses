{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0R2GyY0KmlYW",
        "outputId": "61d05b3d-078d-40bf-f01f-2c174e13da47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Tuple, Union, Callable\n",
        "import json\n",
        "import nltk\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "IMxDrfE9ogZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BogkPoc7pZ3P",
        "outputId": "5395ff2e-a3d1-47e8-bfec-467643ef90cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glue_qqp_dir = '/drive/MyDrive/ml/QQP'\n",
        "glove_path = '/drive/MyDrive/ml/glove.6B.50d.txt'"
      ],
      "metadata": {
        "id": "mqNnBBYzn-IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianKernel(torch.nn.Module):\n",
        "    def __init__(self, mu: float = 1., sigma: float = 1.):\n",
        "        super().__init__()\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.exp(\n",
        "            -0.5 * ((x - self.mu) ** 2) / (self.sigma ** 2)\n",
        "        )\n",
        "\n",
        "\n",
        "class KNRM(torch.nn.Module):\n",
        "    def __init__(self, embedding_matrix: np.ndarray, freeze_embeddings: bool, kernel_num: int = 21,\n",
        "                 sigma: float = 0.1, exact_sigma: float = 0.001,\n",
        "                 out_layers: List[int] = [10, 5]):\n",
        "        super().__init__()\n",
        "        self.embeddings = torch.nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix),\n",
        "            freeze=freeze_embeddings,\n",
        "            padding_idx=0\n",
        "        )\n",
        "\n",
        "        self.kernel_num = kernel_num\n",
        "        self.sigma = sigma\n",
        "        self.exact_sigma = exact_sigma\n",
        "        self.out_layers = out_layers\n",
        "\n",
        "        self.kernels = self._get_kernels_layers()\n",
        "\n",
        "        self.mlp = self._get_mlp()\n",
        "\n",
        "        self.out_activation = torch.nn.Sigmoid()\n",
        "\n",
        "    def _get_kernels_layers(self) -> torch.nn.ModuleList:\n",
        "        kernels = torch.nn.ModuleList()\n",
        "        for i in range(self.kernel_num):\n",
        "            mu = 1. / (self.kernel_num - 1) + (2. * i) / (\n",
        "                self.kernel_num - 1) - 1.0\n",
        "            sigma = self.sigma\n",
        "            if mu > 1.0:\n",
        "                sigma = self.exact_sigma\n",
        "                mu = 1.0\n",
        "            kernels.append(GaussianKernel(mu=mu, sigma=sigma))\n",
        "        return kernels\n",
        "\n",
        "    def _get_mlp(self) -> torch.nn.Sequential:\n",
        "        out_cont = [self.kernel_num] + self.out_layers + [1]\n",
        "        mlp = [\n",
        "            torch.nn.Sequential(\n",
        "                torch.nn.Linear(in_f, out_f),\n",
        "                torch.nn.ReLU()\n",
        "            )\n",
        "            for in_f, out_f in zip(out_cont, out_cont[1:])\n",
        "        ]\n",
        "        mlp[-1] = mlp[-1][:-1]\n",
        "        return torch.nn.Sequential(*mlp)\n",
        "\n",
        "    def forward(self, input_1: Dict[str, torch.Tensor], input_2: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
        "        logits_1 = self.predict(input_1)\n",
        "        logits_2 = self.predict(input_2)\n",
        "\n",
        "        logits_diff = logits_1 - logits_2\n",
        "\n",
        "        out = self.out_activation(logits_diff)\n",
        "        return out\n",
        "\n",
        "    def _get_matching_matrix(self, query: torch.Tensor, doc: torch.Tensor) -> torch.FloatTensor:\n",
        "        # shape = [B, L, D]\n",
        "        embed_query = self.embeddings(query.long())\n",
        "        # shape = [B, R, D]\n",
        "        embed_doc = self.embeddings(doc.long())\n",
        "\n",
        "        # shape = [B, L, R]\n",
        "        matching_matrix = torch.einsum(\n",
        "            'bld,brd->blr',\n",
        "            F.normalize(embed_query, p=2, dim=-1),\n",
        "            F.normalize(embed_doc, p=2, dim=-1)\n",
        "        )\n",
        "        return matching_matrix\n",
        "\n",
        "    def _apply_kernels(self, matching_matrix: torch.FloatTensor) -> torch.FloatTensor:\n",
        "        KM = []\n",
        "        for kernel in self.kernels:\n",
        "            # shape = [B]\n",
        "            K = torch.log1p(kernel(matching_matrix).sum(dim=-1)).sum(dim=-1)\n",
        "            KM.append(K)\n",
        "\n",
        "        # shape = [B, K]\n",
        "        kernels_out = torch.stack(KM, dim=1)\n",
        "        return kernels_out\n",
        "\n",
        "    def predict(self, inputs: Dict[str, torch.Tensor]) -> torch.FloatTensor:\n",
        "        query, doc = inputs['query'], inputs['document']\n",
        "        # shape = [B, L, R]\n",
        "        matching_matrix = self._get_matching_matrix(query, doc)\n",
        "        # shape [B, K]\n",
        "        kernels_out = self._apply_kernels(matching_matrix)\n",
        "        # shape [B]\n",
        "        out = self.mlp(kernels_out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class RankingDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, index_pairs_or_triplets: List[List[Union[str, float]]],\n",
        "                 idx_to_text_mapping: Dict[str, str], vocab: Dict[str, int], oov_val: int,\n",
        "                 preproc_func: Callable, max_len: int = 30):\n",
        "        self.index_pairs_or_triplets = index_pairs_or_triplets\n",
        "        self.idx_to_text_mapping = idx_to_text_mapping\n",
        "        self.vocab = vocab\n",
        "        self.oov_val = oov_val\n",
        "        self.preproc_func = preproc_func\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index_pairs_or_triplets)\n",
        "\n",
        "    def _tokenized_text_to_index(self, tokenized_text: List[str]) -> List[int]:\n",
        "        res = [self.vocab.get(i, self.oov_val) for i in tokenized_text]\n",
        "        return res\n",
        "\n",
        "    def _convert_text_idx_to_token_idxs(self, idx: int) -> List[int]:\n",
        "        tokenized_text = self.preproc_func(self.idx_to_text_mapping[idx])\n",
        "        idxs = self._tokenized_text_to_index(tokenized_text)\n",
        "        return idxs\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        pass\n",
        "\n",
        "\n",
        "class TrainTripletsDataset(RankingDataset):\n",
        "    def __getitem__(self, idx):\n",
        "        cur_row = self.index_pairs_or_triplets[idx]\n",
        "        left_idxs = self._convert_text_idx_to_token_idxs(cur_row[0])[\n",
        "            :self.max_len]\n",
        "        r1_idxs = self._convert_text_idx_to_token_idxs(cur_row[1])[\n",
        "            :self.max_len]\n",
        "        r2_idxs = self._convert_text_idx_to_token_idxs(cur_row[2])[\n",
        "            :self.max_len]\n",
        "\n",
        "        pair_1 = {'query': left_idxs, 'document': r1_idxs}\n",
        "        pair_2 = {'query': left_idxs, 'document': r2_idxs}\n",
        "        target = cur_row[3]\n",
        "        return (pair_1, pair_2, target)\n",
        "\n",
        "\n",
        "class ValPairsDataset(RankingDataset):\n",
        "    def __getitem__(self, idx):\n",
        "        cur_row = self.index_pairs_or_triplets[idx]\n",
        "        left_idxs = self._convert_text_idx_to_token_idxs(cur_row[0])[\n",
        "            :self.max_len]\n",
        "        r1_idxs = self._convert_text_idx_to_token_idxs(cur_row[1])[\n",
        "            :self.max_len]\n",
        "\n",
        "        pair = {'query': left_idxs, 'document': r1_idxs}\n",
        "        target = cur_row[2]\n",
        "        return (pair, target)\n",
        "\n",
        "\n",
        "def collate_fn(batch_objs: List[Union[Dict[str, torch.Tensor], torch.FloatTensor]]):\n",
        "    max_len_q1 = -1\n",
        "    max_len_d1 = -1\n",
        "    max_len_q2 = -1\n",
        "    max_len_d2 = -1\n",
        "\n",
        "    is_triplets = False\n",
        "    for elem in batch_objs:\n",
        "        if len(elem) == 3:\n",
        "            left_elem, right_elem, label = elem\n",
        "            is_triplets = True\n",
        "        else:\n",
        "            left_elem, label = elem\n",
        "\n",
        "        max_len_q1 = max(len(left_elem['query']), max_len_q1)\n",
        "        max_len_d1 = max(len(left_elem['document']), max_len_d1)\n",
        "        if len(elem) == 3:\n",
        "            max_len_q2 = max(len(right_elem['query']), max_len_q2)\n",
        "            max_len_d2 = max(len(right_elem['document']), max_len_d2)\n",
        "\n",
        "    q1s = []\n",
        "    d1s = []\n",
        "    q2s = []\n",
        "    d2s = []\n",
        "    labels = []\n",
        "\n",
        "    for elem in batch_objs:\n",
        "        if is_triplets:\n",
        "            left_elem, right_elem, label = elem\n",
        "        else:\n",
        "            left_elem, label = elem\n",
        "\n",
        "        pad_len1 = max_len_q1 - len(left_elem['query'])\n",
        "        pad_len2 = max_len_d1 - len(left_elem['document'])\n",
        "        if is_triplets:\n",
        "            pad_len3 = max_len_q2 - len(right_elem['query'])\n",
        "            pad_len4 = max_len_d2 - len(right_elem['document'])\n",
        "\n",
        "        q1s.append(left_elem['query'] + [0] * pad_len1)\n",
        "        d1s.append(left_elem['document'] + [0] * pad_len2)\n",
        "        if is_triplets:\n",
        "            q2s.append(right_elem['query'] + [0] * pad_len3)\n",
        "            d2s.append(right_elem['document'] + [0] * pad_len4)\n",
        "        labels.append([label])\n",
        "    q1s = torch.LongTensor(q1s)\n",
        "    d1s = torch.LongTensor(d1s)\n",
        "    if is_triplets:\n",
        "        q2s = torch.LongTensor(q2s)\n",
        "        d2s = torch.LongTensor(d2s)\n",
        "    labels = torch.FloatTensor(labels)\n",
        "\n",
        "    ret_left = {'query': q1s, 'document': d1s}\n",
        "    if is_triplets:\n",
        "        ret_right = {'query': q2s, 'document': d2s}\n",
        "        return ret_left, ret_right, labels\n",
        "    else:\n",
        "        return ret_left, labels\n",
        "\n",
        "\n",
        "class Solution:\n",
        "    def __init__(self, glue_qqp_dir: str, glove_vectors_path: str,\n",
        "                 min_token_occurancies: int = 1,\n",
        "                 random_seed: int = 0,\n",
        "                 emb_rand_uni_bound: float = 0.2,\n",
        "                 freeze_knrm_embeddings: bool = True,\n",
        "                 knrm_kernel_num: int = 21,\n",
        "                 knrm_out_mlp: List[int] = [],\n",
        "                 dataloader_bs: int = 1024,\n",
        "                 train_lr: float = 0.001,\n",
        "                 change_train_loader_ep: int = 10\n",
        "                 ):\n",
        "        self.glue_qqp_dir = glue_qqp_dir\n",
        "        self.glove_vectors_path = glove_vectors_path\n",
        "        self.glue_train_df = self.get_glue_df('train')\n",
        "        self.glue_dev_df = self.get_glue_df('dev')\n",
        "        self.dev_pairs_for_ndcg = self.create_val_pairs(self.glue_dev_df)\n",
        "        self.min_token_occurancies = min_token_occurancies\n",
        "        self.all_tokens = self.get_all_tokens(\n",
        "            [self.glue_train_df, self.glue_dev_df], self.min_token_occurancies)\n",
        "\n",
        "        self.random_seed = random_seed\n",
        "        self.emb_rand_uni_bound = emb_rand_uni_bound\n",
        "        self.freeze_knrm_embeddings = freeze_knrm_embeddings\n",
        "        self.knrm_kernel_num = knrm_kernel_num\n",
        "        self.knrm_out_mlp = knrm_out_mlp\n",
        "        self.dataloader_bs = dataloader_bs\n",
        "        self.train_lr = train_lr\n",
        "        self.change_train_loader_ep = change_train_loader_ep\n",
        "\n",
        "        self.model, self.vocab, self.unk_words = self.build_knrm_model()\n",
        "        self.idx_to_text_mapping_train = self.get_idx_to_text_mapping(\n",
        "            self.glue_train_df)\n",
        "        self.idx_to_text_mapping_dev = self.get_idx_to_text_mapping(\n",
        "            self.glue_dev_df)\n",
        "        \n",
        "        self.val_dataset = ValPairsDataset(self.dev_pairs_for_ndcg, \n",
        "              self.idx_to_text_mapping_dev, \n",
        "              vocab=self.vocab, oov_val=self.vocab['OOV'], \n",
        "              preproc_func=self.simple_preproc)\n",
        "        self.val_dataloader = torch.utils.data.DataLoader(\n",
        "            self.val_dataset, batch_size=self.dataloader_bs, num_workers=0, \n",
        "            collate_fn=collate_fn, shuffle=False)\n",
        "\n",
        "    def get_glue_df(self, partition_type: str):\n",
        "        assert partition_type in ['dev', 'train']\n",
        "        glue_df = pd.read_csv(\n",
        "            self.glue_qqp_dir + f'/{partition_type}.tsv', sep='\\t', error_bad_lines=False, dtype=object)\n",
        "        glue_df = glue_df.dropna(axis=0, how='any').reset_index(drop=True)\n",
        "        glue_df_fin = pd.DataFrame({\n",
        "            'id_left': glue_df['qid1'],\n",
        "            'id_right': glue_df['qid2'],\n",
        "            'text_left': glue_df['question1'],\n",
        "            'text_right': glue_df['question2'],\n",
        "            'label': glue_df['is_duplicate'].astype(int)\n",
        "        })\n",
        "        return glue_df_fin\n",
        "\n",
        "    def hadle_punctuation(self, inp_str: str) -> str:\n",
        "        inp_str = str(inp_str)\n",
        "        for punct in string.punctuation:\n",
        "            inp_str = inp_str.replace(punct, ' ')\n",
        "        return inp_str\n",
        "\n",
        "    def simple_preproc(self, inp_str: str):\n",
        "        base_str = inp_str.strip().lower()\n",
        "        str_wo_punct = self.hadle_punctuation(base_str)\n",
        "        return nltk.word_tokenize(str_wo_punct)\n",
        "\n",
        "    def _filter_rare_words(self, vocab: dict, min_occurancies: int) -> dict:\n",
        "        out_vocab = dict()\n",
        "        for word, cnt in vocab.items():\n",
        "            if cnt >= min_occurancies:\n",
        "                out_vocab[word] = cnt\n",
        "        return out_vocab\n",
        "\n",
        "    def get_all_tokens(self, list_of_df: List[pd.DataFrame], min_occurancies: int) -> List[str]:\n",
        "        def flatten(t): return [item for sublist in t for item in sublist]\n",
        "        tokens = []\n",
        "        for df in list_of_df:\n",
        "            unique_texts = set(\n",
        "                df[['text_left', 'text_right']].values.reshape(-1))\n",
        "            df_tokens = flatten(map(self.simple_preproc, unique_texts))\n",
        "            tokens.extend(list(df_tokens))\n",
        "        count_filtered = self._filter_rare_words(\n",
        "            Counter(tokens), min_occurancies)\n",
        "        return list(count_filtered.keys())\n",
        "\n",
        "    def _read_glove_embeddings(self, file_path: str) -> Dict[str, List[str]]:\n",
        "        embedding_data = {}\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                current_line = line.rstrip().split(' ')\n",
        "                embedding_data[current_line[0]] = current_line[1:]\n",
        "        return embedding_data\n",
        "\n",
        "    def create_glove_emb_from_file(self, file_path: str, inner_keys: List[str],\n",
        "                                   random_seed: int, rand_uni_bound: float\n",
        "                                   ) -> Tuple[np.ndarray, Dict[str, int], List[str]]:\n",
        "        glove_emb = self._read_glove_embeddings(file_path)\n",
        "\n",
        "        inner_keys = ['PAD', 'OOV'] + inner_keys\n",
        "        input_dim = len(inner_keys)\n",
        "        out_dim = len(list(glove_emb.values())[0])\n",
        "        vocab = dict()\n",
        "        np.random.seed(random_seed)\n",
        "        matrix = np.empty((input_dim, out_dim))\n",
        "        unk_words = []\n",
        "\n",
        "        for idx, word in enumerate(inner_keys):\n",
        "            vocab[word] = idx\n",
        "            if word in glove_emb:\n",
        "                matrix[idx] = glove_emb[word]\n",
        "            else:\n",
        "                unk_words.append(word)\n",
        "                matrix[idx] = np.random.uniform(-rand_uni_bound,\n",
        "                                                rand_uni_bound, size=out_dim)\n",
        "        matrix[0] = np.zeros_like(matrix[0])\n",
        "        return matrix, vocab, unk_words\n",
        "\n",
        "    def build_knrm_model(self) -> Tuple[torch.nn.Module, Dict[str, int], List[str]]:\n",
        "        emb_matrix, vocab, unk_words = self.create_glove_emb_from_file(\n",
        "            self.glove_vectors_path, self.all_tokens, self.random_seed, self.emb_rand_uni_bound)\n",
        "        torch.manual_seed(self.random_seed)\n",
        "        knrm = KNRM(emb_matrix, freeze_embeddings=self.freeze_knrm_embeddings,\n",
        "                    out_layers=self.knrm_out_mlp, kernel_num=self.knrm_kernel_num)\n",
        "        return knrm, vocab, unk_words\n",
        "\n",
        "    def sample_data_for_train_iter(self, inp_df: pd.DataFrame, seed: int\n",
        "                                   ) -> List[List[Union[str, float]]]:\n",
        "        groups = inp_df[['id_left', 'id_right', 'label']].groupby('id_left')\n",
        "        pairs_w_labels = []\n",
        "        np.random.seed(seed)\n",
        "        all_right_ids = inp_df.id_right.values\n",
        "        for id_left, group in groups:\n",
        "            labels = group.label.unique()\n",
        "            if len(labels) > 1:\n",
        "                for label in labels:\n",
        "                    same_label_samples = group[group.label ==\n",
        "                                               label].id_right.values\n",
        "                    if label == 0 and len(same_label_samples) > 1:\n",
        "                        sample = np.random.choice(\n",
        "                            same_label_samples, 2, replace=False)\n",
        "                        pairs_w_labels.append(\n",
        "                            [id_left, sample[0], sample[1], 0.5])\n",
        "                    elif label == 1:\n",
        "                        less_label_samples = group[group.label <\n",
        "                                                   label].id_right.values\n",
        "                        pos_sample = np.random.choice(\n",
        "                            same_label_samples, 1, replace=False)\n",
        "                        if len(less_label_samples) > 0:\n",
        "                            neg_sample = np.random.choice(\n",
        "                                less_label_samples, 1, replace=False)\n",
        "                        else:\n",
        "                            neg_sample = np.random.choice(\n",
        "                                all_right_ids, 1, replace=False)\n",
        "                        pairs_w_labels.append(\n",
        "                            [id_left, pos_sample[0], neg_sample[0], 1])\n",
        "        return pairs_w_labels\n",
        "\n",
        "    def create_val_pairs(self, inp_df: pd.DataFrame, fill_top_to: int = 15,\n",
        "                         min_group_size: int = 2, seed: int = 0) -> List[List[Union[str, float]]]:\n",
        "        inp_df_select = inp_df[['id_left', 'id_right', 'label']]\n",
        "        inf_df_group_sizes = inp_df_select.groupby('id_left').size()\n",
        "        glue_dev_leftids_to_use = list(\n",
        "            inf_df_group_sizes[inf_df_group_sizes >= min_group_size].index)\n",
        "        groups = inp_df_select[inp_df_select.id_left.isin(\n",
        "            glue_dev_leftids_to_use)].groupby('id_left')\n",
        "\n",
        "        all_ids = set(inp_df['id_left']).union(set(inp_df['id_right']))\n",
        "\n",
        "        out_pairs = []\n",
        "\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        for id_left, group in groups:\n",
        "            ones_ids = group[group.label > 0].id_right.values\n",
        "            zeroes_ids = group[group.label == 0].id_right.values\n",
        "            sum_len = len(ones_ids) + len(zeroes_ids)\n",
        "            num_pad_items = max(0, fill_top_to - sum_len)\n",
        "            if num_pad_items > 0:\n",
        "                cur_chosen = set(ones_ids).union(\n",
        "                    set(zeroes_ids)).union({id_left})\n",
        "                pad_sample = np.random.choice(\n",
        "                    list(all_ids - cur_chosen), num_pad_items, replace=False).tolist()\n",
        "            else:\n",
        "                pad_sample = []\n",
        "            for i in ones_ids:\n",
        "                out_pairs.append([id_left, i, 2])\n",
        "            for i in zeroes_ids:\n",
        "                out_pairs.append([id_left, i, 1])\n",
        "            for i in pad_sample:\n",
        "                out_pairs.append([id_left, i, 0])\n",
        "        return out_pairs\n",
        "\n",
        "    def get_idx_to_text_mapping(self, inp_df: pd.DataFrame) -> Dict[str, str]:\n",
        "        left_dict = inp_df[['id_left', 'text_left']].drop_duplicates().set_index('id_left')[\n",
        "            'text_left'].to_dict()\n",
        "        right_dict = inp_df[['id_right', 'text_right']].drop_duplicates(\n",
        "        ).set_index('id_right')['text_right'].to_dict()\n",
        "        left_dict.update(right_dict)\n",
        "        return left_dict\n",
        "\n",
        "    def ndcg_k(self, ys_true: np.array, ys_pred: np.array, ndcg_top_k: int = 10) -> float:\n",
        "        def dcg(ys_true, ys_pred):\n",
        "            argsort = np.argsort(ys_pred)[::-1]\n",
        "            argsort = argsort[:ndcg_top_k]\n",
        "            ys_true_sorted = ys_true[argsort]\n",
        "            ret = 0\n",
        "            for i, l in enumerate(ys_true_sorted, 1):\n",
        "                ret += (2 ** l - 1) / math.log2(1 + i)\n",
        "            return ret\n",
        "        ideal_dcg = dcg(ys_true, ys_true)\n",
        "        pred_dcg = dcg(ys_true, ys_pred)\n",
        "        return (pred_dcg / ideal_dcg)\n",
        "\n",
        "    def valid(self, model: torch.nn.Module, val_dataloader: torch.utils.data.DataLoader) -> float:\n",
        "        labels_and_groups = val_dataloader.dataset.index_pairs_or_triplets\n",
        "        labels_and_groups = pd.DataFrame(labels_and_groups, columns=['left_id', 'right_id', 'rel'])\n",
        "        \n",
        "        all_preds = []\n",
        "        for batch in (val_dataloader):\n",
        "            inp_1, y = batch\n",
        "            preds = model.predict(inp_1)\n",
        "            preds_np = preds.detach().numpy()\n",
        "            all_preds.append(preds_np)\n",
        "        all_preds = np.concatenate(all_preds, axis=0)\n",
        "        labels_and_groups['preds'] = all_preds\n",
        "        \n",
        "        ndcgs = []\n",
        "        for cur_id in labels_and_groups.left_id.unique():\n",
        "            cur_df = labels_and_groups[labels_and_groups.left_id == cur_id]\n",
        "            ndcg = self.ndcg_k(cur_df.rel.values.reshape(-1), cur_df.preds.values.reshape(-1))\n",
        "            if np.isnan(ndcg):\n",
        "                ndcgs.append(0)\n",
        "            else:\n",
        "                ndcgs.append(ndcg)\n",
        "        return np.mean(ndcgs)\n",
        "\n",
        "    def train(self, n_epochs: int):\n",
        "        opt = torch.optim.SGD(self.model.parameters(), lr=self.train_lr)\n",
        "        criterion = torch.nn.BCELoss()\n",
        "        ndcgs = []\n",
        "        for ep in range(n_epochs):\n",
        "            if ep % self.change_train_loader_ep == 0:\n",
        "                sampled_train_triplets = self.sample_data_for_train_iter(self.glue_train_df, seed = ep)\n",
        "                train_dataset = TrainTripletsDataset(sampled_train_triplets, \n",
        "                        self.idx_to_text_mapping_train, \n",
        "                        vocab=self.vocab, oov_val=self.vocab['OOV'], \n",
        "                        preproc_func=self.simple_preproc)\n",
        "                train_dataloader = torch.utils.data.DataLoader(\n",
        "                    train_dataset, batch_size=self.dataloader_bs, num_workers=0, \n",
        "                    collate_fn=collate_fn, shuffle=True, )\n",
        "\n",
        "            for batch in (train_dataloader):\n",
        "\n",
        "                inp_1, inp_2, y = batch\n",
        "                preds = self.model(inp_1, inp_2)\n",
        "                loss = criterion(preds, y)\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "            ndcg = self.valid(self.model, self.val_dataloader)\n",
        "            ndcgs.append(ndcg)\n",
        "            if ndcg > 0.925:\n",
        "                break"
      ],
      "metadata": {
        "id": "gOTM9C_7ozDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sol = Solution(glue_qqp_dir, glove_path, knrm_out_mlp=[])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96ue6OwXpEHY",
        "outputId": "2d529b01-5726-42f3-90c2-87720e5eaf6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-23e9b020f843>:233: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  self.glue_train_df = self.get_glue_df('train')\n",
            "<ipython-input-5-23e9b020f843>:234: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  self.glue_dev_df = self.get_glue_df('dev')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sol.train(10)"
      ],
      "metadata": {
        "id": "saxU-dP9pHcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state_mlp = sol.model.mlp.state_dict()\n",
        "torch.save(state_mlp, open('/drive/MyDrive/ml/knrm_mlp.bin', 'wb'))\n",
        "\n",
        "state_emb = sol.model.embeddings.state_dict()\n",
        "torch.save(state_emb, open('/drive/MyDrive/ml/knrm_emb.bin', 'wb'))\n",
        "\n",
        "state_vocab = sol.vocab\n",
        "json.dump(state_vocab, open('/drive/MyDrive/ml/vocab.json', 'w', encoding='utf-8'), ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "7crmx1htuPcW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}