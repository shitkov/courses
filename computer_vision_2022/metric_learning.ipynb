{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_xj5YPPWVh4"
   },
   "source": [
    "# Metric Learning: Prototypical Network for Few-Shot Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD9QIsigzhLZ"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/brendenlake/omniglot/raw/master/python/images_evaluation.zip\n",
    "!wget https://github.com/brendenlake/omniglot/raw/master/python/images_background.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rm2zbpLuWqTw"
   },
   "outputs": [],
   "source": [
    "!unzip -qq images_background.zip\n",
    "!unzip -qq images_evaluation.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDHEkkQIXmEW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from skimage import io\n",
    "\n",
    "from scipy import ndimage\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZm9TcuPYidr"
   },
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRws9xUsYlXt"
   },
   "outputs": [],
   "source": [
    "def read_alphabets(alphabet_directory_path, alphabet_directory_name):\n",
    "    \"\"\"\n",
    "    Reads all the characters from a given alphabet_directory\n",
    "    \"\"\"\n",
    "    datax = []\n",
    "    datay = []\n",
    "    characters = os.listdir(alphabet_directory_path)\n",
    "    for character in characters:\n",
    "        images = os.listdir(alphabet_directory_path + character + '/')\n",
    "        for img in images:\n",
    "            image = cv2.resize(\n",
    "                cv2.imread(alphabet_directory_path + character + '/' + img),\n",
    "                (28,28)\n",
    "                )\n",
    "            #rotations of image\n",
    "            rotated_90 = ndimage.rotate(image, 90)\n",
    "            rotated_180 = ndimage.rotate(image, 180)\n",
    "            rotated_270 = ndimage.rotate(image, 270)\n",
    "            datax.extend((image, rotated_90, rotated_180, rotated_270))\n",
    "            datay.extend((\n",
    "                alphabet_directory_name + '_' + character + '_0',\n",
    "                alphabet_directory_name + '_' + character + '_90',\n",
    "                alphabet_directory_name + '_' + character + '_180',\n",
    "                alphabet_directory_name + '_' + character + '_270'\n",
    "            ))\n",
    "    return np.array(datax), np.array(datay)\n",
    "\n",
    "def read_images(base_directory):\n",
    "    \"\"\"\n",
    "    Reads all the alphabets from the base_directory\n",
    "    Uses multithreading to decrease the reading time drastically\n",
    "    \"\"\"\n",
    "    datax = None\n",
    "    datay = None\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    results = [pool.apply(read_alphabets,\n",
    "                          args=(\n",
    "                              base_directory + '/' + directory + '/', directory, \n",
    "                              )) for directory in os.listdir(base_directory)]\n",
    "    pool.close()\n",
    "    for result in results:\n",
    "        if datax is None:\n",
    "            datax = result[0]\n",
    "            datay = result[1]\n",
    "        else:\n",
    "            datax = np.vstack([datax, result[0]])\n",
    "            datay = np.concatenate([datay, result[1]])\n",
    "    return datax, datay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1U6ifg05Yv7V"
   },
   "outputs": [],
   "source": [
    "class OmniglotDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_dir):\n",
    "      self.dataset_dir = f'{dataset_dir}'\n",
    "      self.images, self.labels = read_images(self.dataset_dir)\n",
    "\n",
    "      encoder = LabelEncoder()\n",
    "      self.labels = encoder.fit_transform(self.labels)\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        data = torch.tensor(self.images[index])\n",
    "        data = data.permute(2,0,1)\n",
    "        data = data.type(torch.FloatTensor)\n",
    "        label = torch.tensor(self.labels[index])\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPmalKMlYx-6"
   },
   "outputs": [],
   "source": [
    "train_dataset = OmniglotDataset('/content/images_background')\n",
    "valid_dataset = OmniglotDataset('/content/images_evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBoyP_fMY8Su"
   },
   "outputs": [],
   "source": [
    "class CustomSampler(Sampler):\n",
    "\n",
    "    def __init__(self, labels, n_batch, n_ways, n_support, n_query):\n",
    "        self.n_batch = n_batch # training episodes (количество батчей на 1 эпоху)\n",
    "        self.n_ways = n_ways # количество классов\n",
    "        self.n_shots = n_support # количество support векторов в одном классе\n",
    "        self.n_query = n_query # количество query векторов в одном классе\n",
    "        self.n_elmts = n_support + n_query\n",
    "\n",
    "        unique_labels = np.unique(labels)\n",
    "\n",
    "        self.indices_per_class = {idx: np.where(labels == idx)[0] for idx in unique_labels}\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        for _ in range(self.n_batch):\n",
    "            batch = []\n",
    "\n",
    "            # 1.1. choose random classes\n",
    "            n_classes = len(self.indices_per_class) # number of unique classes\n",
    "            classes = np.random.choice(n_classes, self.n_ways, replace=False)\n",
    "\n",
    "            # 1.2. save indices of elements for randomly chosen classes\n",
    "            for class_k in classes:\n",
    "              # indexes of elements for class k\n",
    "                indices_k = self.indices_per_class[class_k]\n",
    "\n",
    "                # choose random elements inside class k\n",
    "                n_elements = len(indices_k) \n",
    "                pos = np.random.choice(n_elements, self.n_elmts, replace=False)\n",
    "\n",
    "                # save indices of chosen elements into batch\n",
    "                batch.append(indices_k[pos])\n",
    "            \n",
    "            # from 2d array to 1d array of indices      # class 1: [element 1, element 2], class 2: [element 1, element 2], ...]\n",
    "            \n",
    "            batch = np.stack(batch, axis=-1).reshape(1,-1)[0]\n",
    "            # batch = batch.numpy()\n",
    "\n",
    "            yield batch # на выходе: i - class, j - element's number  [x11, x21, x12, x22]\n",
    "\n",
    "    def __len__(self): # количество элементов в итераторе\n",
    "        return self.n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSoeaGo_epGx"
   },
   "outputs": [],
   "source": [
    "train_sampler = CustomSampler(labels = train_dataset.labels,\n",
    "                                    n_batch = 2000,\n",
    "                                    n_ways = 60, # n_way\n",
    "                                    n_support = 5, # n_shots\n",
    "                                    n_query = 5)\n",
    "\n",
    "val_sampler = CustomSampler(labels = valid_dataset.labels, \n",
    "                                    n_batch = 60,\n",
    "                                    n_ways = 5, \n",
    "                                    n_support = 5, \n",
    "                                    n_query = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ln5qLBdfeqIK"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_dataset, \n",
    "                          batch_sampler=train_sampler,\n",
    "                          num_workers=2, pin_memory=True)\n",
    "\n",
    "val_dataloader = DataLoader(dataset=valid_dataset, \n",
    "                        batch_sampler=val_sampler,\n",
    "                        num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVLSNPPoHxdv"
   },
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChzPu5j3-BTI"
   },
   "outputs": [],
   "source": [
    "class Convnet(nn.Module):\n",
    "\n",
    "    def __init__(self, x_dim=(3,28,28), hid_dim=64, z_dim=64):\n",
    "        super(Convnet, self).__init__()\n",
    "        \n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2)\n",
    "            )\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(x_dim[0], hid_dim),\n",
    "            conv_block(hid_dim, hid_dim),\n",
    "            conv_block(hid_dim, hid_dim),\n",
    "            conv_block(hid_dim, z_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dbaqwgZIseQ"
   },
   "outputs": [],
   "source": [
    "model = Convnet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdUJoZJoK3_6"
   },
   "source": [
    "## 3. Optimizer + Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQqPsvHIIszf"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIfLjV1AK9MO"
   },
   "source": [
    "## 4. Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVY4O7fyK28r"
   },
   "outputs": [],
   "source": [
    "def calculate_dist(query, prototype):\n",
    "        '''\n",
    "        Calculate squared euclidean distance between each sample and prototype \n",
    "        from each class.\n",
    "        i.e.\n",
    "        Q = [q11, # shape: n_query*n_ways, emb_size qij - i - class, j - element\n",
    "            q21,\n",
    "            q31, \n",
    "            q12,\n",
    "            q22,\n",
    "            q32]\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        query: 2d tensor with shape [n_query*n_ways, emb_size]\n",
    "        prototype: 2d tensor with shape [n_ways, emb_size]\n",
    "        dist_type: distance type to calculate. \n",
    "                   It can be either \"squared_euclidean\" or \"cosine_similarity\".\n",
    "        \n",
    "        Return\n",
    "        -----------\n",
    "        distances: 2d tensor with shape torch.Size([n_query*n_ways, n_ways])\n",
    "            d(q11, c1), d(q11, c2), d(q11, c3)\n",
    "            d(q21, c1), d(q21, c2), d(q21, c3)\n",
    "            d(q31, c1), d(q31, c2), d(q31, c3)\n",
    "\n",
    "            d(q12, c1), d(q12, c2), d(q11, c3)\n",
    "            d(q22, c1), d(q22, c2), d(q21, c3)\n",
    "            d(q32, c1), d(q32, c2), d(q32, c3)\n",
    "\n",
    "        '''\n",
    "        # x: N x D\n",
    "        # y: M x D\n",
    "        n = query.size(0)\n",
    "        m = prototype.size(0)\n",
    "        d = query.size(1)\n",
    "\n",
    "        # with shape [n_query*n_ways, n_ways, emb_size]\n",
    "        query = query.unsqueeze(1).expand(n, m, d) \n",
    "        prototype = prototype.unsqueeze(0).expand(n, m, d) \n",
    "        \n",
    "        distance = torch.pow(query - prototype, 2).sum(2)\n",
    "        \n",
    "        return distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24-MGKw9QFO-"
   },
   "outputs": [],
   "source": [
    "class PrototypicalLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PrototypicalLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, data, labels, n_ways, n_shots, n_query):\n",
    "\n",
    "        # 1. Divide data on support and query\n",
    "        p = n_ways * n_shots\n",
    "        support = data[:p,:]\n",
    "        query = data[p:,:]\n",
    "\n",
    "        # 1.1. make shape [n_shots, n_ways, emb_size]\n",
    "        support = support.reshape(n_shots, n_ways, -1)\n",
    "\n",
    "        # 2. Compute prototype from support examples for each class\n",
    "        prototype = support.mean(0)\n",
    "\n",
    "        # 3. Compute euclidean distances between query samples and prototypes\n",
    "        dist = calculate_dist(query, prototype)\n",
    "\n",
    "        # 4. Calculate CrossEntopyLoss(-d(q,ck))\n",
    "        logits = -dist\n",
    "        log_p_y = F.log_softmax(-dist, dim=1).view(n_ways, n_query, -1)\n",
    "        \n",
    "        target_inds = torch.arange(0, n_ways)\n",
    "        target_inds = target_inds.view(n_ways, 1, 1)\n",
    "        target_inds = target_inds.expand(n_ways, n_query, 1).long()\n",
    "\n",
    "        loss = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n",
    "\n",
    "        _, y_hat = log_p_y.max(2)\n",
    "        acc = y_hat.eq(target_inds.squeeze(2)).float().mean()\n",
    "\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPNH4mVGQGl_"
   },
   "outputs": [],
   "source": [
    "criterion = PrototypicalLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkrwyHj3S9FD"
   },
   "source": [
    "## 5. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwJ-bC-Bz3FZ"
   },
   "outputs": [],
   "source": [
    "def train_singe_epoch(model,\n",
    "                      train_dataloader, \n",
    "                      epoch, \n",
    "                      n_ways, \n",
    "                      n_shots, \n",
    "                      n_query):\n",
    "    model.train()\n",
    "    pbar = tqdm(train_dataloader, desc=f'Train (epoch = {epoch})', leave=False)  \n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    for batch in pbar:\n",
    "\n",
    "        data, _ = batch \n",
    "        data = data.to(device)\n",
    "        data = model(data)\n",
    "\n",
    "        label = torch.arange(n_ways).repeat(n_query)\n",
    "        label = label.to(device)\n",
    "\n",
    "        loss, acc = criterion(data, label, n_ways, n_shots, n_query)\n",
    "\n",
    "        total_loss += loss\n",
    "        total_acc += acc\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(\"\\nAverage train loss: {}\".format(avg_loss))\n",
    "\n",
    "    avg_acc = total_acc / len(train_dataloader)\n",
    "    print(\"\\nAverage train accuracy: {}\".format(avg_acc))\n",
    "\n",
    "    return model, avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMzXs-GkTCNA"
   },
   "source": [
    "## 6. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y3F_ye02TBvz"
   },
   "outputs": [],
   "source": [
    "def evaluate_single_epoch(model, \n",
    "                          val_dataloader, \n",
    "                          epoch,\n",
    "                          n_ways, \n",
    "                          n_shots, \n",
    "                          n_query):\n",
    "    model.eval()\n",
    "    pbar = tqdm(val_dataloader, desc=f'Validation (epoch = {epoch})', leave=False)  \n",
    "\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    for batch in pbar:\n",
    "\n",
    "        data, _ = batch \n",
    "        data = data.to(device)\n",
    "        data = model(data)\n",
    "\n",
    "        label = torch.arange(n_ways).repeat(n_query)\n",
    "        label = label.to(device)\n",
    "\n",
    "        loss, acc = criterion(data, label, n_ways, n_shots, n_query)\n",
    "\n",
    "        total_loss += loss\n",
    "        total_acc += acc\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_dataloader)\n",
    "    print(\"\\nAverage validation loss: {}\".format(avg_loss))\n",
    "\n",
    "    avg_acc = total_acc / len(val_dataloader)\n",
    "    print(\"\\nAverage validation accuracy: {}\".format(avg_acc))\n",
    "\n",
    "    return model, avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSdtz3bNTcKh"
   },
   "source": [
    "## 7. Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fal6UM0XTYjh"
   },
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                num_epochs, \n",
    "                train_dataloader, \n",
    "                valid_dataloader,\n",
    "                train_sampler,\n",
    "                val_sampler):\n",
    "\n",
    "    logs = {}\n",
    "    logs['train_loss'] = []\n",
    "    logs['train_acc'] = []\n",
    "    logs['val_loss'] = []\n",
    "    logs['val_acc'] = []\n",
    "    logs['best_acc'] = 0.0\n",
    "\n",
    "    for epoch in trange(num_epochs, desc=\"Epoch\"):\n",
    "        \n",
    "        model, train_loss, train_acc = train_singe_epoch(model, \n",
    "                                  train_dataloader,\n",
    "                                  epoch, \n",
    "                                  train_sampler.n_ways, \n",
    "                                  train_sampler.n_shots, \n",
    "                                  train_sampler.n_query)\n",
    "        \n",
    "        model, val_loss, val_acc = evaluate_single_epoch(model, \n",
    "                                  valid_dataloader,\n",
    "                                  epoch, \n",
    "                                  val_sampler.n_ways, \n",
    "                                  val_sampler.n_shots, \n",
    "                                  val_sampler.n_query)\n",
    "        scheduler.step()\n",
    "\n",
    "        logs['train_loss'].append(train_loss)\n",
    "        logs['train_acc'].append(train_acc)\n",
    "        logs['val_loss'].append(val_loss)\n",
    "        logs['val_acc'].append(val_acc)\n",
    "\n",
    "        if logs['best_acc'] < val_acc:\n",
    "            logs['best_acc'] = val_acc\n",
    "\n",
    "        torch.save(logs,'logs')\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Rd1dSTrTfOv"
   },
   "outputs": [],
   "source": [
    "model = train_model(model=model,\n",
    "    num_epochs=5,\n",
    "    train_dataloader=train_dataloader, \n",
    "    valid_dataloader=val_dataloader,\n",
    "    train_sampler=train_sampler,\n",
    "    val_sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UreEQDmTgi2"
   },
   "outputs": [],
   "source": [
    "logs = torch.load(\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4kREnCzTlaW"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(logs[\"train_loss\"])),logs[\"train_loss\"])\n",
    "plt.title(\"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Prototypical Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2YtHl3fTmrC"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(logs[\"train_acc\"])),logs[\"train_acc\"])\n",
    "plt.title(\"Train Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xmEmiYL7Tn7j"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(logs[\"val_loss\"])),logs[\"val_loss\"])\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Prototypical Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpqZ6cCaTrKI"
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(logs[\"val_acc\"])),logs[\"val_acc\"])\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\");"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
