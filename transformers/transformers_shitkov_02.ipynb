{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwUNiF7zwf+vCmp23q1ylF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shitkov/courses/blob/master/transformers/transformers_shitkov_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b09d7a19-5848-43f4-9d91-f35d4e8614b0"
      },
      "source": [
        "# 1. Information about the submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e37cb5bb-f3d0-4c11-a1dc-2490a208fcd3"
      },
      "source": [
        "## 1.1 Name and number of the assignment "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e9d00b8-f3e5-4a44-bcc6-35cdd60767a9"
      },
      "source": [
        "DEtoxification - #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64ba7f63-66ec-4691-a5d2-17f4679e298d"
      },
      "source": [
        "## 1.2 Student name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc8a4e09-62cc-43fd-a7a7-3e9d55ec13b2"
      },
      "source": [
        "Konstantin Shitkov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a46ab45-d215-41af-b910-63ff4a215a07"
      },
      "source": [
        "## 1.3 Codalab user ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b15cd6b5-8e20-4287-b6ea-a7b0904b355a"
      },
      "source": [
        "shitkov"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70456c74-8e1f-4da0-bebe-fbceee169115"
      },
      "source": [
        "## 1.4 Additional comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b810ac6-7739-4f7f-8bea-dbf1198570ea"
      },
      "source": [
        "-"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1af498ab-3c00-4d36-a962-c947862fede8"
      },
      "source": [
        "# 2. Technical Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "061f71b9-114a-4cb0-b531-5711970317bf"
      },
      "source": [
        "## 2.1 Methodology "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline: T5\n",
        "\n",
        "Experiments:\n",
        "1.   ruT5-base\n",
        "1.   ruT5-large + extended data"
      ],
      "metadata": {
        "id": "e6Y4xzMhgHQL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afe27e49-10c7-4c12-adea-48b0a05a5681"
      },
      "source": [
        "## 2.2 Discussion of results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline: T5\n",
        "\n",
        "Experiments:\n",
        "1.   ruT5-base: 0.47\n",
        "1.   ruT5-large + extended data: 0.53\n",
        "\n",
        "Увеличить модель и добавить данных - всегда работатет. Данные расширил путем обратной токсификации с использованием модели IlyaGusev/rut5_tox, а также перевода датасета parallel_detoxification_dataset_small.tsv на русский с фильтрацией: классифицировал с помощью целевого берта и выбирал пары токс/детокс."
      ],
      "metadata": {
        "id": "uLB3JuhmgIJf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "194fecf1-e044-4210-a54b-aefbf4b4eebe"
      },
      "source": [
        "# 3. Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "metadata": {
        "id": "PW2HRlCDCiDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b3c19fa-f883-4675-9506-85c4f02f0af9"
      },
      "source": [
        "## 3.1 Download the data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/s-nlp/russe_detox_2022/main/data/input/train.tsv\n",
        "!wget https://raw.githubusercontent.com/s-nlp/russe_detox_2022/main/data/input/dev.tsv\n",
        "!wget https://raw.githubusercontent.com/s-nlp/russe_detox_2022/main/data/input/test.tsv\n",
        "!wget https://raw.githubusercontent.com/s-nlp/parallel_detoxification_dataset/main/parallel_detoxification_dataset_small.tsv\n",
        "\n",
        "!wget https://raw.githubusercontent.com/s-nlp/russe_detox_2022/main/evaluation/ru_detoxification_evaluation.py\n",
        "!wget https://raw.githubusercontent.com/s-nlp/russe_detox_2022/main/evaluation/ru_detoxification_metrics.py"
      ],
      "metadata": {
        "id": "7gxTPjVwCW3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Requirements"
      ],
      "metadata": {
        "id": "qWmSmorMrSa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install pytorch-crf\n",
        "!pip install transformers sentencepiece\n",
        "!pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "P543yot6ofgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from googletrans import Translator\n",
        "\n",
        "from typing import Tuple, List, Dict, Union\n",
        "\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import T5ForConditionalGeneration, AutoTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
        "\n",
        "from ru_detoxification_evaluation import load_model\n",
        "from ru_detoxification_metrics import evaluate_style\n",
        "from ru_detoxification_metrics import evaluate_cosine_similarity\n",
        "from ru_detoxification_metrics import evaluate_cola_relative"
      ],
      "metadata": {
        "id": "c7aeagNoC4OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanup():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "MJtwYxgz7Iky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = True"
      ],
      "metadata": {
        "id": "GEpFWH6pJ0_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Translate"
      ],
      "metadata": {
        "id": "4dJFfA8hDZBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('parallel_detoxification_dataset_small.tsv', sep='\\t')\n",
        "\n",
        "toxic_comments = list(data['toxic_comment'])\n",
        "civil_comments = list(data['civil_comment'])"
      ],
      "metadata": {
        "id": "MGCv44QXXRun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator()"
      ],
      "metadata": {
        "id": "snSiNHxHXU6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_comments_ru = []\n",
        "civil_comments_ru = []\n",
        "for tox, detox in tqdm(zip(toxic_comments, civil_comments)):\n",
        "    toxic_comments_ru.append(translator.translate(tox, dest='ru').text)\n",
        "    civil_comments_ru.append(translator.translate(detox, dest='ru').text)"
      ],
      "metadata": {
        "id": "O5PoOoUqYClc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Filtering translated data"
      ],
      "metadata": {
        "id": "UtjJ9jb0EGOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "style_model, style_tokenizer = load_model('SkolkovoInstitute/russian_toxicity_classifier', use_cuda=use_cuda)"
      ],
      "metadata": {
        "id": "nb337Ig4D-fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toxic_comments_ru_labels = evaluate_style(\n",
        "    model = style_model,\n",
        "    tokenizer = style_tokenizer,\n",
        "    texts = toxic_comments_ru,\n",
        "    target_label=1,  # 1 is toxic, 0 is neutral\n",
        "    batch_size=32, \n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "chYgoWiCJ9v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "civil_comments_ru_labels = evaluate_style(\n",
        "    model = style_model,\n",
        "    tokenizer = style_tokenizer,\n",
        "    texts = civil_comments_ru,\n",
        "    target_label=0,  # 1 is toxic, 0 is neutral\n",
        "    batch_size=1, \n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "_3b-CfeZKYbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame()\n",
        "\n",
        "data['toxic_comment'] = toxic_comments_ru\n",
        "data['civil_comment'] = civil_comments_ru\n",
        "\n",
        "data['toxic_label'] = toxic_comments_ru_labels\n",
        "data['civil_label'] = civil_comments_ru_labels\n",
        "\n",
        "df_translated = data[(data['toxic_label'] > .5) & (data['civil_label'] > .5)].drop(columns=['toxic_label','civil_label'])"
      ],
      "metadata": {
        "id": "i5b0aI-eLBB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Retoxify civil comments"
      ],
      "metadata": {
        "id": "A1rhktIrM2A_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('train.tsv', sep='\\t', index_col='index')\n",
        "df = df.fillna('')"
      ],
      "metadata": {
        "id": "q5S_aNyuiNwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_toxic = []\n",
        "df_train_neutral = []\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    references = row[['neutral_comment1', 'neutral_comment2', 'neutral_comment3']].tolist()\n",
        "    \n",
        "    for reference in references:\n",
        "        if len(reference) > 0:\n",
        "            df_train_toxic.append(row['toxic_comment'])\n",
        "            df_train_neutral.append(reference)\n",
        "        else:\n",
        "            break"
      ],
      "metadata": {
        "id": "qECw1ChMi-0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.DataFrame()\n",
        "df_train['toxic_comment'] = df_train_toxic\n",
        "df_train['civil_comment'] = df_train_neutral"
      ],
      "metadata": {
        "id": "zZpq6iVwNOeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'IlyaGusev/rut5_tox'\n",
        "\n",
        "model_tox = T5ForConditionalGeneration.from_pretrained(model_name).cuda()\n",
        "tokenizer_tox = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "50m_oKXsOrtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def paraphrase_tox(text, model, tokenizer, n=None, max_length='auto', temperature=0.0, beams=3):\n",
        "    texts = [text] if isinstance(text, str) else text\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True)['input_ids'].to(model.device)\n",
        "    if max_length == 'auto':\n",
        "        max_length = int(inputs.shape[1] * 1.2) + 10\n",
        "    result = model.generate(\n",
        "        inputs, \n",
        "        num_return_sequences=n or 1, \n",
        "        do_sample=False, \n",
        "        temperature=temperature, \n",
        "        repetition_penalty=3.0, \n",
        "        max_length=max_length,\n",
        "        bad_words_ids=[[2]],  # unk\n",
        "        num_beams=beams,\n",
        "    )\n",
        "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
        "    if not n and isinstance(text, str):\n",
        "        return texts[0]\n",
        "    return texts"
      ],
      "metadata": {
        "id": "PqpyhxSYO0RG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retox = [paraphrase_tox(text, model_tox, tokenizer_tox) for text in tqdm(df_train_neutral)]"
      ],
      "metadata": {
        "id": "DalDzerGO36C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_retox = pd.DataFrame()\n",
        "df_retox['toxic_comment'] = retox\n",
        "df_retox['civil_comment'] = df_train_neutral"
      ],
      "metadata": {
        "id": "30I-CWWJPBmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model_tox\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "FbnzGIEwPRC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retox_labels = evaluate_style(\n",
        "    model = style_model,\n",
        "    tokenizer = style_tokenizer,\n",
        "    texts = retox,\n",
        "    target_label=1,  # 1 is toxic, 0 is neutral\n",
        "    batch_size=32, \n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "dxRtss40PYu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_retox['retox_labels'] = retox_labels"
      ],
      "metadata": {
        "id": "Ok_eIfYfQgqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_retox = df_retox[df_retox['retox_labels'] > .5]"
      ],
      "metadata": {
        "id": "-pQNlJ7GQs07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del style_model\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "YkRRmUTtSfhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.memory_allocated() // 1073741824)"
      ],
      "metadata": {
        "id": "MgATARTCTHwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Add dev data"
      ],
      "metadata": {
        "id": "mvuwgMjhRBUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev = pd.read_csv('dev.tsv', sep='\\t')"
      ],
      "metadata": {
        "id": "Tb2npSLdQ1ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev = dev.fillna('')"
      ],
      "metadata": {
        "id": "b_w-v_3VRK86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_toxic = []\n",
        "df_train_neutral = []\n",
        "\n",
        "for index, row in dev.iterrows():\n",
        "    references = row[['neutral_comment1', 'neutral_comment2', 'neutral_comment3']].tolist()\n",
        "    \n",
        "    for reference in references:\n",
        "        if len(reference) > 0:\n",
        "            df_train_toxic.append(row['toxic_comment'])\n",
        "            df_train_neutral.append(reference)\n",
        "        else:\n",
        "            break"
      ],
      "metadata": {
        "id": "V4lAqtMRRN2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dev = pd.DataFrame()\n",
        "df_dev['toxic_comment'] = df_train_toxic\n",
        "df_dev['civil_comment'] = df_train_neutral"
      ],
      "metadata": {
        "id": "A2pSYms_RWfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df_train, df_dev, df_translated, df_retox])\n",
        "df = df.sample(frac=1)\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ui983RJbRjTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('df.csv')"
      ],
      "metadata": {
        "id": "sS7pJjsnSAP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('df.csv')"
      ],
      "metadata": {
        "id": "2c62K8u0XwUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Train"
      ],
      "metadata": {
        "id": "O9jaWEbXTL4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PairsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx < len(self.x['input_ids'])\n",
        "        item = {key: val[idx] for key, val in self.x.items()}\n",
        "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
        "        item['labels'] = self.y['input_ids'][idx]\n",
        "        return item\n",
        "    \n",
        "    @property\n",
        "    def n(self):\n",
        "        return len(self.x['input_ids'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n # * 2"
      ],
      "metadata": {
        "id": "QPO1gKnzSaFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataCollatorWithPadding:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        batch = self.tokenizer.pad(\n",
        "            features,\n",
        "            padding=True,\n",
        "        )\n",
        "        ybatch = self.tokenizer.pad(\n",
        "            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n",
        "            padding=True,\n",
        "        ) \n",
        "        batch['labels'] = ybatch['input_ids']\n",
        "        batch['decoder_attention_mask'] = ybatch['attention_mask']\n",
        "        \n",
        "        return {k: torch.tensor(v) for k, v in batch.items()}"
      ],
      "metadata": {
        "id": "CYHrlT8lS8bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(\n",
        "    model, train_dataloader,\n",
        "    max_epochs=10,\n",
        "    lr=3e-5,\n",
        "    gradient_accumulation_steps=1, \n",
        "    cleanup_step=100\n",
        "):\n",
        "    cleanup()\n",
        "    optimizer = torch.optim.Adam(params = [p for p in model.parameters() if p.requires_grad], lr=lr)\n",
        "\n",
        "    step = 0\n",
        "    model.train()\n",
        "\n",
        "    for epoch in trange(max_epochs):\n",
        "        tq = tqdm(train_dataloader)\n",
        "        for i, batch in enumerate(tq):\n",
        "            try:\n",
        "                batch['labels'][batch['labels']==0] = -100\n",
        "                loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss\n",
        "                loss.backward()\n",
        "            except Exception as e:\n",
        "                print('error on step', i, e)\n",
        "                loss = None\n",
        "                cleanup()\n",
        "                continue           \n",
        "                \n",
        "            if i and i % gradient_accumulation_steps == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            if i % cleanup_step == 0:\n",
        "                cleanup()\n",
        "                \n",
        "        model.save_pretrained(f't5_detox')\n",
        "    cleanup()"
      ],
      "metadata": {
        "id": "G52lMuwIUkE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'sberbank-ai/ruT5-base'\n",
        "\n",
        "batch_size = 2"
      ],
      "metadata": {
        "id": "wvy4tIXNUoiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "v8IetZKuUrW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "JAkSi_43UucM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PairsDataset(tokenizer(df['toxic_comment'].tolist()), tokenizer(df['civil_comment'].tolist()))"
      ],
      "metadata": {
        "id": "CSuLlz3RUxz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "7aS_OQ3SZIbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=False, shuffle=True, collate_fn=data_collator)"
      ],
      "metadata": {
        "id": "4JoBfe27ZKW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loop(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    max_epochs=10, \n",
        "    lr=3e-5,\n",
        "    gradient_accumulation_steps=1, \n",
        "    cleanup_step=100\n",
        ")"
      ],
      "metadata": {
        "id": "K5x3GcPCZMYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Test"
      ],
      "metadata": {
        "id": "0uaOiT9daxP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('test.tsv', sep='\\t')\n",
        "toxic_comments = data['toxic_comment'].tolist()"
      ],
      "metadata": {
        "id": "IDX9m1ZUa077"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def paraphrase(text, model, tokenizer):\n",
        "    inputs = tokenizer([text], return_tensors='pt', padding=True)['input_ids'].to(model.device)\n",
        "    \n",
        "    max_length = int(inputs.shape[1] * 1.2) + 10\n",
        "\n",
        "    result = model.generate(\n",
        "        inputs, \n",
        "        num_return_sequences=3, \n",
        "        do_sample=True, \n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=3.0, \n",
        "        max_length=max_length,\n",
        "        bad_words_ids=[[2]],  # unk\n",
        "        num_beams=5,\n",
        "    )\n",
        "\n",
        "    texts = [tokenizer.decode(r, skip_special_tokens=True) for r in result]\n",
        "\n",
        "    return texts"
      ],
      "metadata": {
        "id": "hDWU5UpXZOeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "para_results = []\n",
        "\n",
        "for text in tqdm(toxic_comments):\n",
        "    try:\n",
        "        para_results.append(paraphrase(text, model, tokenizer))\n",
        "    except Exception as e:\n",
        "        print('ERROR')\n",
        "        break"
      ],
      "metadata": {
        "id": "4pH2FvAXahJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "T6VhBcqSbONy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Filtering"
      ],
      "metadata": {
        "id": "qPwBiBLcbLT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 3 # num_return_sequences\n",
        "\n",
        "id_list = []\n",
        "para_x3 = []\n",
        "for i, batch in enumerate(para_results):\n",
        "    para_x3 += batch\n",
        "    id_list += [i] * 3"
      ],
      "metadata": {
        "id": "W3dbdZ3XamYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame()\n",
        "df['id'] = id_list\n",
        "df['text'] = para_x3"
      ],
      "metadata": {
        "id": "YkgxvzZVbvn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1 CLS score"
      ],
      "metadata": {
        "id": "AH7R2Q93b--V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')\n",
        "model = BertForSequenceClassification.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')"
      ],
      "metadata": {
        "id": "ZgTZki5Db4yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cls_score(model, tokenizer, text):\n",
        "    with torch.no_grad():\n",
        "        inputs = tokenizer.encode(text, return_tensors='pt').to(model.device)\n",
        "        logits = model(inputs).logits\n",
        "        score = torch.softmax(logits, -1)[:, 0].cpu().numpy()[0]\n",
        "    return score"
      ],
      "metadata": {
        "id": "T3ymR1pOcM0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cls_score = [get_cls_score(model, tokenizer, text) for text in tqdm(para_x3)] "
      ],
      "metadata": {
        "id": "0a0wJkGicOlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cls'] = cls_score"
      ],
      "metadata": {
        "id": "RC6IyWxkcSRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "BT1cBCo6cayZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.2 COSINE similarity"
      ],
      "metadata": {
        "id": "JR3ufEx6cdMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "meaning_model, meaning_tokenizer = load_model('cointegrated/LaBSE-en-ru', use_cuda=True, model_class=AutoModel)"
      ],
      "metadata": {
        "id": "DM5k_XQVcctb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cos_score = []\n",
        "\n",
        "for i, text in enumerate(para_x3):\n",
        "    x = toxic_comments[int(i//3)]\n",
        "    y = text\n",
        "\n",
        "    similarity = evaluate_cosine_similarity(\n",
        "        model = meaning_model,\n",
        "        tokenizer = meaning_tokenizer,\n",
        "        original_texts = [x],\n",
        "        rewritten_texts = [y],\n",
        "        batch_size=32,\n",
        "        verbose=False,\n",
        "        )[0]\n",
        "    cos_score.append(similarity)"
      ],
      "metadata": {
        "id": "nUhlnA2fcmaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['cos'] = cos_score"
      ],
      "metadata": {
        "id": "knhuY2eXcqFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del meaning_model\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "lPuyL_rfdCrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3 FLUENCY score"
      ],
      "metadata": {
        "id": "jj1OZ34rdLYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cola_model, cola_tolenizer = load_model('SkolkovoInstitute/rubert-base-corruption-detector', use_cuda=True)"
      ],
      "metadata": {
        "id": "Ule-flGadHEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fl_score = []\n",
        "\n",
        "for i, text in enumerate(para_x3):\n",
        "    x = toxic_comments[int(i//3)]\n",
        "    y = text\n",
        "\n",
        "    fluency = evaluate_cola_relative(\n",
        "        model = cola_model,\n",
        "        tokenizer = cola_tolenizer,\n",
        "        original_texts = [x],\n",
        "        rewritten_texts = [y],\n",
        "        target_label=1,\n",
        "        batch_size=32,\n",
        "        verbose=False\n",
        "    )\n",
        "    fl_score.append(fluency[0])"
      ],
      "metadata": {
        "id": "FiG2nwEVdR1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['fl'] = fl_score"
      ],
      "metadata": {
        "id": "Wx9OsVFwdXpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del cola_model\n",
        "cleanup()"
      ],
      "metadata": {
        "id": "ih8ElsM0ddyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.4 MAX J score"
      ],
      "metadata": {
        "id": "-WWfYvfUdi_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['J'] = list(np.array(df['fl'].tolist()) * np.array(df['cos'].tolist()) * np.array(df['cls'].tolist()))"
      ],
      "metadata": {
        "id": "Pltc9eNGdiQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicts_clean = []\n",
        "for id in range(len(toxic_comments)):\n",
        "    data = df[df['id'] == id]\n",
        "    idx = data['J'].idxmax()\n",
        "    predicts_clean.append(data['text'][idx])"
      ],
      "metadata": {
        "id": "9Ufjyy2Rdp-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Save results"
      ],
      "metadata": {
        "id": "CO_ZbP3keGVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('test.txt', 'w') as file:\n",
        "    file.writelines([sentence+'\\n' for sentence in predicts_clean])"
      ],
      "metadata": {
        "id": "BwfiDA-5dw3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r test_final.zip test.txt"
      ],
      "metadata": {
        "id": "4cJKth9JeLnS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}