{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"a5_1_Word2Vec.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"8r69eu8wJvXM","colab_type":"text"},"source":["# Задание 5.1 - Word2Vec\n","\n","В этом задании мы натренируем свои word vectors на очень небольшом датасете.\n","Мы будем использовать самую простую версию word2vec, без negative sampling и других оптимизаций.\n","\n","Перед запуском нужно запустить скрипт `download_data.sh` чтобы скачать данные.\n","\n","Датасет и модель очень небольшие, поэтому это задание можно выполнить и без GPU."]},{"cell_type":"code","metadata":{"id":"8a1jL41OJ23A","colab_type":"code","outputId":"d03d218d-3211-448f-e874-4ea703e77ac6","executionInfo":{"status":"ok","timestamp":1590127686733,"user_tz":-180,"elapsed":25534,"user":{"displayName":"Константин Шитьков","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCkCq5x25vNYZBSH3vcpBiuv_VP9JI0RltsyLT-g=s64","userId":"04248095687395188930"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WvmDWbdeKe2E","colab_type":"code","colab":{}},"source":["!pip3 install -q torch torchvision"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ebsltMmMKrwH","colab_type":"code","colab":{}},"source":["from torchvision import transforms"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-w_RzkVjKZ4H","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B9l-ai1SKv1K","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rT6g0rOpKxXt","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsMvDDDWKxgT","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XUQV3TxPLRxc","colab_type":"code","colab":{}},"source":["import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"njqy3FmDyaLL","colab_type":"code","outputId":"048a6709-3172-490c-c8a6-26ee1268275b","executionInfo":{"status":"ok","timestamp":1590127706284,"user_tz":-180,"elapsed":14863,"user":{"displayName":"Константин Шитьков","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCkCq5x25vNYZBSH3vcpBiuv_VP9JI0RltsyLT-g=s64","userId":"04248095687395188930"}},"colab":{"base_uri":"https://localhost:8080/","height":281}},"source":["!wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip"],"execution_count":0,"outputs":[{"output_type":"stream","text":["--2020-05-22 06:08:19--  http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip [following]\n","--2020-05-22 06:08:19--  https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 6372817 (6.1M) [application/zip]\n","Saving to: ‘stanfordSentimentTreebank.zip’\n","\n","stanfordSentimentTr 100%[===================>]   6.08M  1.46MB/s    in 4.2s    \n","\n","2020-05-22 06:08:24 (1.46 MB/s) - ‘stanfordSentimentTreebank.zip’ saved [6372817/6372817]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Pzi6wNGByfIr","colab_type":"code","colab":{}},"source":["!unzip -q stanfordSentimentTreebank.zip"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rbOkJ8Zfykhm","colab_type":"code","colab":{}},"source":["# Let's make sure GPU is available!\n","device = torch.device(\"cuda:0\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2JLL2jAeyns_","colab_type":"code","colab":{}},"source":["folder = '/content/stanfordSentimentTreebank'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"SpEVNudDJvXj","colab_type":"code","colab":{}},"source":["class StanfordTreeBank:\n","    '''\n","    Wrapper for accessing Stanford Tree Bank Dataset\n","    https://nlp.stanford.edu/sentiment/treebank.html\n","    \n","    Parses dataset, gives each token and index and provides lookups\n","    from string token to index and back\n","    \n","    Allows to generate random context with sampling strategy described in\n","    word2vec paper:\n","    https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n","    '''\n","    def __init__(self):\n","        self.index_by_token = {}\n","        self.token_by_index = []\n","\n","        self.sentences = []\n","\n","        self.token_freq = {}\n","        \n","        self.token_reject_by_index = None\n","\n","    def load_dataset(self, folder):\n","        filename = os.path.join(folder, \"datasetSentences.txt\")\n","\n","        with open(filename, \"r\", encoding=\"latin1\") as f:\n","            l = f.readline() # skip the first line\n","            \n","            for l in f:\n","                splitted_line = l.strip().split()\n","                words = [w.lower() for w in splitted_line[1:]] # First one is a number\n","                    \n","                self.sentences.append(words)\n","                for word in words:\n","                    if word in self.token_freq:\n","                        self.token_freq[word] +=1 \n","                    else:\n","                        index = len(self.token_by_index)\n","                        self.token_freq[word] = 1\n","                        self.index_by_token[word] = index\n","                        self.token_by_index.append(word)\n","        self.compute_token_prob()\n","                        \n","    def compute_token_prob(self):\n","        words_count = np.array([self.token_freq[token] for token in self.token_by_index])\n","        words_freq = words_count / np.sum(words_count)\n","        \n","        # Following sampling strategy from word2vec paper:\n","        # https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\n","        self.token_reject_by_index = 1- np.sqrt(1e-5/words_freq)\n","    \n","    def check_reject(self, word):\n","        return np.random.rand() > self.token_reject_by_index[self.index_by_token[word]]\n","        \n","    def get_random_context(self, context_length=5):\n","        \"\"\"\n","        Returns tuple of center word and list of context words\n","        \"\"\"\n","        sentence_sampled = []\n","        while len(sentence_sampled) <= 2:\n","            sentence_index = np.random.randint(len(self.sentences)) \n","            sentence = self.sentences[sentence_index]\n","            sentence_sampled = [word for word in sentence if self.check_reject(word)]\n","    \n","        center_word_index = np.random.randint(len(sentence_sampled))\n","        \n","        words_before = sentence_sampled[max(center_word_index - context_length//2,0):center_word_index]\n","        words_after = sentence_sampled[center_word_index+1: center_word_index+1+context_length//2]\n","        \n","        return sentence_sampled[center_word_index], words_before+words_after\n","    \n","    def num_tokens(self):\n","        return len(self.token_by_index)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-0HhzbmzCc5","colab_type":"code","outputId":"316e051d-f5ef-49b2-9ca1-442e85d213c6","executionInfo":{"status":"ok","timestamp":1590127708643,"user_tz":-180,"elapsed":16152,"user":{"displayName":"Константин Шитьков","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCkCq5x25vNYZBSH3vcpBiuv_VP9JI0RltsyLT-g=s64","userId":"04248095687395188930"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["data = StanfordTreeBank()\n","data.load_dataset(folder)\n","\n","print(\"Num tokens:\", data.num_tokens())\n","for i in range(5):\n","    center_word, other_words = data.get_random_context(5)\n","    print(center_word, other_words)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Num tokens: 19538\n","alternate ['disappointment', 'versions', 'particularly']\n","interesting ['leads', 'bland']\n","drown ['distinctly', 'sub-par', 'boredom', 'than']\n","historical ['fused', 'triangle']\n","thought ['chuckle', 'ancient', 'librarian']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"luO98r0FJvXz","colab_type":"code","outputId":"9bae55fb-2760-4aa3-f407-6dd0f0d10c79","executionInfo":{"status":"ok","timestamp":1590127709071,"user_tz":-180,"elapsed":15874,"user":{"displayName":"Константин Шитьков","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCkCq5x25vNYZBSH3vcpBiuv_VP9JI0RltsyLT-g=s64","userId":"04248095687395188930"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["class Word2VecPlain(Dataset):\n","    '''\n","    PyTorch Dataset for plain Word2Vec.\n","    Accepts StanfordTreebank as data and is able to generate dataset based on\n","    a number of random contexts\n","    '''\n","    def __init__(self, data, num_contexts=30000):\n","        '''\n","        Initializes Word2VecPlain, but doesn't generate the samples yet\n","        (for that, use generate_dataset)\n","        Arguments:\n","        data - StanfordTreebank instace\n","        num_contexts - number of random contexts to use when generating a dataset\n","        '''\n","        # TODO: Implement what you need for other methods!\n","        self.data = data\n","        self.num_contexts = num_contexts\n","        self.dataset = []\n","        self.num_tokens = self.data.num_tokens()\n","    \n","    def generate_dataset(self, len_context=5):\n","        '''\n","        Generates dataset samples from random contexts\n","        Note: there will be more samples than contexts because every context\n","        can generate more than one sample\n","        '''\n","        # TODO: Implement generating the dataset\n","        # You should sample num_contexts contexts from the data and turn them into samples\n","        # Note you will have several samples from one context\n","        \n","        for i in range(self.num_contexts):\n","            sample = self.data.get_random_context(len_context)\n","            for word in sample[1]:\n","                self.dataset.append((sample[0], word))\n","        \n","    def __len__(self):\n","        '''\n","        Returns total number of samples\n","        '''\n","        # TODO: Return the number of samples\n","        return len(self.dataset)\n","    \n","    def __getitem__(self, index):\n","        '''\n","        Returns i-th sample\n","        \n","        Return values:\n","        input_vector - torch.Tensor with one-hot representation of the input vector\n","        output_index - index of the target word (not torch.Tensor!)\n","        '''\n","        # TODO: Generate tuple of 2 return arguments for i-th sample\n","        word_in, word_out = self.dataset[index]\n","        input_index = self.data.index_by_token[word_in]\n","        output_index = self.data.index_by_token[word_out]\n","        input_vector = np.zeros(self.num_tokens)\n","        input_vector[input_index] = 1\n","        input_vector = torch.tensor(input_vector, dtype=torch.double)\n","        return input_vector, output_index\n","\n","dataset = Word2VecPlain(data, 10)\n","dataset.generate_dataset(5)\n","input_vector, target = dataset[3]\n","print(\"Sample - input: %s, target: %s\" % (input_vector, int(target))) # target should be able to convert to int\n","assert isinstance(input_vector, torch.Tensor)\n","assert torch.sum(input_vector) == 1.0\n","assert input_vector.shape[0] == data.num_tokens()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Sample - input: tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64), target: 5789\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qn0Vt8RjJvX9","colab_type":"text"},"source":["# Создаем модель и тренируем ее"]},{"cell_type":"code","metadata":{"id":"iIOJWwrkJvX-","colab_type":"code","outputId":"eb3bb61b-5b0b-4722-aaae-be6661e033be","executionInfo":{"status":"ok","timestamp":1590127710193,"user_tz":-180,"elapsed":15975,"user":{"displayName":"Константин Шитьков","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiCkCq5x25vNYZBSH3vcpBiuv_VP9JI0RltsyLT-g=s64","userId":"04248095687395188930"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["# Create the usual PyTorch structures\n","dataset = Word2VecPlain(data, 30000)\n","dataset.generate_dataset()\n","\n","# We'll be training very small word vectors!\n","wordvec_dim = 10\n","\n","# We can use a standard sequential model for this\n","nn_model = nn.Sequential(\n","            nn.Linear(dataset.num_tokens, wordvec_dim, bias=False),\n","            nn.Linear(wordvec_dim, dataset.num_tokens, bias=False), \n","         )\n","nn_model.type(torch.FloatTensor)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=19538, out_features=10, bias=False)\n","  (1): Linear(in_features=10, out_features=19538, bias=False)\n",")"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"DcDv6P6KJvYI","colab_type":"code","colab":{}},"source":["def extract_word_vectors(nn_model):\n","    '''\n","    Extracts word vectors from the model\n","    \n","    Returns:\n","    input_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n","    output_vectors: torch.Tensor with dimensions (num_tokens, num_dimensions)\n","    '''\n","    # TODO: Implement extracting word vectors from param weights\n","    # return tuple of input vectors and output vectos \n","    # Hint: you can access weights as Tensors through nn.Linear class attributes\n","    return nn_model[0].weight.T, nn_model[1].weight\n","\n","untrained_input_vectors, untrained_output_vectors = extract_word_vectors(nn_model)\n","assert untrained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n","assert untrained_output_vectors.shape == (data.num_tokens(), wordvec_dim)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqcY4OQZJvYQ","colab_type":"code","colab":{}},"source":["def train_model(model, dataset, train_loader, optimizer, scheduler, num_epochs):\n","    '''\n","    Trains plain word2vec using cross-entropy loss and regenerating dataset every epoch\n","    \n","    Returns:\n","    loss_history, train_history\n","    '''\n","    loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n","    \n","    loss_history = []\n","    train_history = []\n","\n","    for epoch in range(num_epochs):\n","        model.train() # Enter train mode     \n","        dataset.generate_dataset() # Regenerate dataset every epoch\n","        loss_accum = 0\n","        correct_samples = 0\n","        total_samples = 0\n","\n","        for i_step, (x, y) in enumerate(train_loader):\n","            \n","            x_gpu = x.to(device)\n","            y_gpu = y.to(device)\n","\n","            prediction = model(x_gpu.float())\n","\n","            loss_value = loss(prediction, y_gpu)\n","\n","            optimizer.zero_grad()\n","            loss_value.backward()\n","            optimizer.step()\n","\n","            _, idx = torch.max(prediction, 1)\n","            \n","            correct_samples += torch.sum(idx == y_gpu)\n","            total_samples += y.shape[0]\n","            loss_accum += loss_value \n","\n","        scheduler.step()\n","\n","        ave_loss = loss_accum / i_step\n","        train_accuracy = float(correct_samples) / total_samples\n","        \n","        loss_history.append(float(ave_loss))\n","        train_history.append(train_accuracy)\n","\n","        print(\"Epoch %i, Average loss: %f, Train accuracy: %f\" % (epoch, ave_loss, train_accuracy))\n","        \n","    return loss_history, train_history"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JfrdDHdFJvYZ","colab_type":"text"},"source":["# Ну и наконец тренировка!\n","\n","Добейтесь значения ошибки меньше **8.0**."]},{"cell_type":"code","metadata":{"id":"tt2Nv-8gJvYa","colab_type":"code","colab":{}},"source":["# Finally, let's train the model!\n","nn_model.to(device)\n","# TODO: We use placeholder values for hyperparameters - you will need to find better values!\n","optimizer = optim.SGD(nn_model.parameters(), lr=1e-1, weight_decay=0)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=20)\n","\n","loss_history, train_history = train_model(nn_model, dataset, train_loader, optimizer, scheduler, 10)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2yyAIgxCBJh6","colab_type":"code","colab":{}},"source":["import random\n","lr_list = [.1, .01, .001, .0001]\n","weight_decay_list = [1e-5, 1e-7, 1e-9]\n","step_size_list = [10, 100]\n","gamma_list = [.1, .5, .9]\n","batch_size_list = [10, 50, 100]\n","\n","nn_model.to(device)\n","\n","epoch = 5\n","for i in range(100):\n","    lr = np.random.choice(lr_list, replace=False)\n","    weight_decay = np.random.choice(weight_decay_list, replace=False)\n","\n","    optimizer_list = [\n","                optim.SGD(nn_model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9),\n","                optim.Adam(nn_model.parameters(), lr=lr, weight_decay=weight_decay)\n","                ]\n","\n","    step_size = np.random.choice(step_size_list, replace=False)\n","    gamma = np.random.choice(gamma_list, replace=False)\n","    batch_size = int(np.random.choice(batch_size_list, replace=False))\n","    optimizer = np.random.choice(optimizer_list, replace=False)\n","    \n","    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n","    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","    \n","    print('=================')\n","    print(i)\n","    print(lr)\n","    print(weight_decay)\n","    print(step_size)\n","    print(gamma)\n","    print(batch_size)\n","    print(optimizer)\n","    loss_history, train_history = train_model(nn_model, dataset, train_loader, optimizer, scheduler, epoch)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6CaSy-R-Kln","colab_type":"code","colab":{}},"source":["nn_model = nn.Sequential(\n","            nn.Linear(dataset.num_tokens, wordvec_dim, bias=False),\n","            nn.Linear(wordvec_dim, dataset.num_tokens, bias=False), \n","         )\n","\n","nn_model.to(device)\n","\n","epoch = 10\n","\n","lr = 0.01\n","weight_decay = 1e-09\n","step_size = 20\n","gamma = 0.9\n","batch_size = 64\n","optimizer = optim.Adam(nn_model.parameters(), lr=lr, weight_decay=weight_decay)\n","\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n","train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n","\n","loss_history, train_history = train_model(nn_model, dataset, train_loader, optimizer, scheduler, epoch)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r-TsSuaBJvYg","colab_type":"code","colab":{}},"source":["# Visualize training graphs\n","plt.subplot(211)\n","plt.plot(train_history)\n","plt.subplot(212)\n","plt.plot(loss_history)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tH00YLcEJvYm","colab_type":"text"},"source":["# Визуализируем вектора для разного вида слов до и после тренировки\n","\n","В случае успешной тренировки вы должны увидеть как вектора слов разных типов (например, знаков препинания, предлогов и остальных) разделяются семантически.\n","\n","Студенты - в качестве выполненного задания присылайте notebook с диаграммами!"]},{"cell_type":"code","metadata":{"id":"zWwoS6F__vM1","colab_type":"code","colab":{}},"source":["trained_input_vectors, trained_output_vectors = extract_word_vectors(nn_model)\n","assert trained_input_vectors.shape == (data.num_tokens(), wordvec_dim)\n","assert trained_output_vectors.shape == (data.num_tokens(), wordvec_dim)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYjco50QJvYo","colab_type":"code","colab":{}},"source":["def visualize_vectors(input_vectors, output_vectors, title=''):\n","    full_vectors = torch.cat((input_vectors, output_vectors), 0)\n","    full_vectors = full_vectors.cpu().detach().numpy()\n","    wordvec_embedding = PCA(n_components=2).fit_transform(full_vectors)\n","\n","    # Helpful words form CS244D example\n","    # http://cs224d.stanford.edu/assignment1/index.html\n","    visualize_words = {'green': [\"the\", \"a\", \"an\"], \n","                      'blue': [\",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\"], \n","                      'brown': [\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \n","                              \"well\", \"amazing\", \"worth\", \"sweet\", \"enjoyable\"],\n","                      'orange': [\"boring\", \"bad\", \"waste\", \"dumb\", \"annoying\", \"stupid\"],\n","                      'red': ['tell', 'told', 'said', 'say', 'says', 'tells', 'goes', 'go', 'went']\n","                     }\n","\n","    plt.figure(figsize=(7,7))\n","    plt.suptitle(title)\n","    for color, words in visualize_words.items():\n","        points = np.array([wordvec_embedding[data.index_by_token[w]] for w in words])\n","        for i, word in enumerate(words):\n","            plt.text(points[i, 0], points[i, 1], word, color=color,horizontalalignment='center')\n","        plt.scatter(points[:, 0], points[:, 1], c=color, alpha=0.3, s=0.5)\n","\n","visualize_vectors(untrained_input_vectors, untrained_output_vectors, \"Untrained word vectors\")\n","visualize_vectors(trained_input_vectors, trained_output_vectors, \"Trained word vectors\")"],"execution_count":0,"outputs":[]}]}